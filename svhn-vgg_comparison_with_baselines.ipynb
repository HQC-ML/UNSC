{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "import utils\n",
    "from agents import *\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seeds',          type=int,       default=[2023, 2024, 2025])\n",
    "parser.add_argument('--dataset',        type=str,       default='svhn')\n",
    "parser.add_argument('--batch_size',     type=int,       default=256)\n",
    "parser.add_argument('--model_name',     type=str,       default='vgg11')\n",
    "parser.add_argument('--retrain',        type=bool,      default=False)\n",
    "parser.add_argument('--unlearn_class',  type=list,      default=3)\n",
    "args = parser.parse_args(\"\")\n",
    "args.time_str = time.strftime(\"%m-%d-%H-%M\", time.localtime())\n",
    "if args.dataset.lower() == 'fmnist':\n",
    "    args.n_channels = 1\n",
    "else:\n",
    "    args.n_channels = 3\n",
    "\n",
    "if args.dataset.lower() == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "else:\n",
    "    args.num_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlearn_dataloader(data_loader):\n",
    "    dataset = data_loader.dataset\n",
    "    _indices = data_loader.sampler.indices\n",
    "\n",
    "    if args.dataset.lower() == 'svhn':\n",
    "        train_targets = np.array(dataset.labels)[_indices]\n",
    "    else:\n",
    "        train_targets = np.array(dataset.labels)[_indices]\n",
    "    unlearn_indices, remain_indices = [], []\n",
    "    for i, target in enumerate(train_targets):\n",
    "        if target in args.unlearn_class:\n",
    "            unlearn_indices.append(i)\n",
    "        else:\n",
    "            remain_indices.append(i)\n",
    "\n",
    "    unlearn_indices = np.array(_indices)[unlearn_indices]\n",
    "    remain_indices = np.array(_indices)[remain_indices]\n",
    "\n",
    "    unlearn_sampler = torch.utils.data.SubsetRandomSampler(unlearn_indices)\n",
    "    unlearn_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler = unlearn_sampler,)\n",
    "\n",
    "    remain_sampler = torch.utils.data.SubsetRandomSampler(remain_indices)\n",
    "    remain_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler = remain_sampler)\n",
    "    return remain_loader, unlearn_loader\n",
    "\n",
    "def get_dataloader(args):\n",
    "    train_loader, test_loader = utils.get_dataloader(args)\n",
    "\n",
    "    indices = np.arange(len(train_loader.dataset))\n",
    "    a = np.split(indices,[int(len(indices)*0.9), int(len(indices))])\n",
    "    idx_train = a[0]\n",
    "    idx_val = a[1]\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(idx_train)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(idx_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_loader.dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler=train_sampler)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(train_loader.dataset,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                sampler=val_sampler)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_loader.dataset,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False)\n",
    "    \n",
    "    if args.retrain:\n",
    "        train_loader, _ = get_unlearn_dataloader(train_loader)\n",
    "        val_loader, _ = get_unlearn_dataloader(val_loader)\n",
    "        \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = 'original_model'\n",
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "train_targets_list = np.array(train_loader.dataset.labels)[train_loader.sampler.indices]\n",
    "unlearn_indices = np.where(np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "\n",
    "# conver to the original indices\n",
    "unlearn_indices = train_loader.sampler.indices[unlearn_indices]\n",
    "\n",
    "unlearn_sampler = torch.utils.data.SubsetRandomSampler(unlearn_indices)\n",
    "unlearn_subset_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                                    batch_size=args.batch_size, \n",
    "                                                    sampler=unlearn_sampler)\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)\n",
    "\n",
    "remain_indices = np.where(~np.isin(train_targets_list, args.unlearn_class))[0]\n",
    "remain_indices = train_loader.sampler.indices[remain_indices]\n",
    "\n",
    "remain_sampler = torch.utils.data.SubsetRandomSampler(remain_indices)\n",
    "remain_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            sampler=remain_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "------------ Retrained model ------------\n",
      "0.9358, 0.9123, 0.9605, 0.0000, 0.9172, 0.9304, 0.9530, 0.9064, 0.8530, 0.8884, Acc_f: 0.0000, Acc_r: 0.9174\n",
      "0.9713, 0.9722, 0.9687, 0.0000, 0.9738, 0.9643, 0.9565, 0.9371, 0.9301, 0.9524, Acc_f: 0.0000, Acc_r: 0.9585\n",
      "0.9576, 0.9702, 0.9684, 0.0000, 0.9362, 0.9186, 0.8801, 0.8722, 0.9229, 0.9455, Acc_f: 0.0000, Acc_r: 0.9302\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "print('------------ Retrained model ------------')\n",
    "for i in range(3):\n",
    "    model_r = get_model(args)\n",
    "    try:\n",
    "        model_r.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/retrain_model_{args.seeds[i]}.pth'))\n",
    "        test_by_class(model_r, test_loader, i=args.unlearn_class)\n",
    "    except:\n",
    "        print('No retrained model')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model Acc_f: 93.23 $\\pm$ 0.91\n",
      "Original model Acc_r: 95.39 $\\pm$ 0.34\n",
      "Retrained model: 93.54 $\\pm$ 1.72\n"
     ]
    }
   ],
   "source": [
    "Acc_f = 100*np.array([0.9362, 0.9198, 0.9410])\n",
    "Acc_r = 100*np.array([0.9549, 0.9574, 0.9493])\n",
    "print(f'Original model Acc_f: {Acc_f.mean():.2f} $\\pm$ {Acc_f.std():.2f}')\n",
    "print(f'Original model Acc_r: {Acc_r.mean():.2f} $\\pm$ {Acc_r.std():.2f}')\n",
    "\n",
    "Acc_r = 100*np.array([0.9174, 0.9585, 0.9302])\n",
    "print(f'Retrained model: {Acc_r.mean():.2f} $\\pm$ {Acc_r.std():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 29, loss 4.423434734344482\n",
      "0.9685, 0.9761, 0.9706, 0.7429, 0.9639, 0.9572, 0.9393, 0.9326, 0.9229, 0.9166, Acc_f: 0.7429, Acc_r: 0.9497\n",
      "[train] epoch 1, batch 29, loss 2.9884297847747803\n",
      "0.9604, 0.9773, 0.9585, 0.1582, 0.9358, 0.9241, 0.8862, 0.8375, 0.8349, 0.7674, Acc_f: 0.1582, Acc_r: 0.8980\n",
      "[train] epoch 2, batch 29, loss 2.679971933364868\n",
      "0.9547, 0.9737, 0.9450, 0.0392, 0.9267, 0.8972, 0.8467, 0.7568, 0.7645, 0.6903, Acc_f: 0.0392, Acc_r: 0.8617\n",
      "[train] epoch 3, batch 29, loss 2.4780831336975098\n",
      "0.9518, 0.9686, 0.9356, 0.0139, 0.9187, 0.8888, 0.8265, 0.7023, 0.7151, 0.6276, Acc_f: 0.0139, Acc_r: 0.8372\n",
      "[train] epoch 4, batch 29, loss 2.443204164505005\n",
      "0.9490, 0.9639, 0.9262, 0.0059, 0.9152, 0.8872, 0.8159, 0.6647, 0.6783, 0.5824, Acc_f: 0.0059, Acc_r: 0.8203\n",
      "[train] epoch 5, batch 29, loss 2.341336250305176\n",
      "0.9472, 0.9594, 0.9159, 0.0028, 0.9045, 0.8918, 0.8134, 0.6305, 0.6572, 0.5505, Acc_f: 0.0028, Acc_r: 0.8078\n",
      "[train] epoch 6, batch 29, loss 2.384671449661255\n",
      "0.9461, 0.9539, 0.9058, 0.0010, 0.8989, 0.9031, 0.8139, 0.6067, 0.6470, 0.5172, Acc_f: 0.0010, Acc_r: 0.7992\n",
      "[train] epoch 7, batch 29, loss 2.3626928329467773\n",
      "0.9450, 0.9476, 0.9009, 0.0010, 0.8950, 0.9035, 0.8199, 0.5909, 0.6392, 0.5085, Acc_f: 0.0010, Acc_r: 0.7945\n",
      "[train] epoch 8, batch 29, loss 2.310896635055542\n",
      "0.9444, 0.9410, 0.8920, 0.0007, 0.8894, 0.9073, 0.8331, 0.5790, 0.6367, 0.4928, Acc_f: 0.0007, Acc_r: 0.7906\n",
      "[train] epoch 9, batch 29, loss 2.3340563774108887\n",
      "0.9421, 0.9321, 0.8814, 0.0007, 0.8843, 0.9107, 0.8366, 0.5651, 0.6301, 0.4784, Acc_f: 0.0007, Acc_r: 0.7845\n",
      "[train] epoch 10, batch 29, loss 2.2745201587677\n",
      "0.9409, 0.9282, 0.8691, 0.0003, 0.8787, 0.9245, 0.8422, 0.5518, 0.6295, 0.4470, Acc_f: 0.0003, Acc_r: 0.7791\n",
      "[train] epoch 11, batch 29, loss 2.299481153488159\n",
      "0.9415, 0.9227, 0.8559, 0.0000, 0.8708, 0.9245, 0.8477, 0.5483, 0.6319, 0.4376, Acc_f: 0.0000, Acc_r: 0.7757\n",
      "[train] epoch 12, batch 29, loss 2.301882743835449\n",
      "0.9415, 0.9165, 0.8539, 0.0000, 0.8652, 0.9270, 0.8513, 0.5389, 0.6307, 0.4257, Acc_f: 0.0000, Acc_r: 0.7723\n",
      "[train] epoch 13, batch 29, loss 2.247100591659546\n",
      "0.9369, 0.9088, 0.8501, 0.0000, 0.8573, 0.9274, 0.8467, 0.5319, 0.6271, 0.4257, Acc_f: 0.0000, Acc_r: 0.7680\n",
      "[train] epoch 14, batch 29, loss 2.2550745010375977\n",
      "0.9364, 0.9076, 0.8349, 0.0000, 0.8589, 0.9262, 0.8513, 0.5225, 0.6283, 0.4006, Acc_f: 0.0000, Acc_r: 0.7630\n",
      "[train] epoch 15, batch 29, loss 2.24252986907959\n",
      "0.9358, 0.9078, 0.8192, 0.0000, 0.8577, 0.9241, 0.8477, 0.5191, 0.6277, 0.3837, Acc_f: 0.0000, Acc_r: 0.7581\n",
      "[train] epoch 16, batch 29, loss 2.268402338027954\n",
      "0.9335, 0.9033, 0.8163, 0.0000, 0.8569, 0.9220, 0.8427, 0.5176, 0.6241, 0.3793, Acc_f: 0.0000, Acc_r: 0.7551\n",
      "[train] epoch 17, batch 29, loss 2.2644705772399902\n",
      "0.9352, 0.9010, 0.8091, 0.0000, 0.8585, 0.9136, 0.8452, 0.5201, 0.6235, 0.3737, Acc_f: 0.0000, Acc_r: 0.7533\n",
      "[train] epoch 18, batch 29, loss 2.2544596195220947\n",
      "0.9323, 0.8961, 0.7951, 0.0000, 0.8502, 0.9119, 0.8412, 0.5191, 0.6241, 0.3768, Acc_f: 0.0000, Acc_r: 0.7496\n",
      "[train] epoch 19, batch 29, loss 2.243344783782959\n",
      "0.9329, 0.8923, 0.7906, 0.0000, 0.8530, 0.9065, 0.8397, 0.5191, 0.6229, 0.3718, Acc_f: 0.0000, Acc_r: 0.7476\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n",
      "[train] epoch 0, batch 29, loss 4.879380226135254\n",
      "0.9656, 0.9751, 0.9704, 0.8001, 0.9616, 0.9685, 0.9509, 0.9396, 0.9060, 0.9536, Acc_f: 0.8001, Acc_r: 0.9546\n",
      "[train] epoch 1, batch 29, loss 3.4527530670166016\n",
      "0.9599, 0.9812, 0.9431, 0.3595, 0.9532, 0.9329, 0.9090, 0.9302, 0.8187, 0.9197, Acc_f: 0.3595, Acc_r: 0.9275\n",
      "[train] epoch 2, batch 29, loss 2.9858601093292236\n",
      "0.9581, 0.9823, 0.8949, 0.1697, 0.9536, 0.8985, 0.8665, 0.9272, 0.7108, 0.8382, Acc_f: 0.1697, Acc_r: 0.8923\n",
      "[train] epoch 3, batch 29, loss 2.7506394386291504\n",
      "0.9541, 0.9816, 0.8614, 0.0857, 0.9548, 0.8796, 0.8295, 0.9237, 0.6247, 0.7498, Acc_f: 0.0857, Acc_r: 0.8621\n",
      "[train] epoch 4, batch 29, loss 2.625410318374634\n",
      "0.9513, 0.9804, 0.8289, 0.0458, 0.9560, 0.8695, 0.8037, 0.9208, 0.5380, 0.6746, Acc_f: 0.0458, Acc_r: 0.8359\n",
      "[train] epoch 5, batch 29, loss 2.5227794647216797\n",
      "0.9472, 0.9798, 0.7968, 0.0194, 0.9572, 0.8633, 0.7785, 0.9168, 0.4855, 0.6245, Acc_f: 0.0194, Acc_r: 0.8166\n",
      "[train] epoch 6, batch 29, loss 2.4910898208618164\n",
      "0.9455, 0.9788, 0.7710, 0.0111, 0.9572, 0.8624, 0.7613, 0.9153, 0.4404, 0.5774, Acc_f: 0.0111, Acc_r: 0.8010\n",
      "[train] epoch 7, batch 29, loss 2.5003483295440674\n",
      "0.9415, 0.9786, 0.7467, 0.0052, 0.9560, 0.8612, 0.7415, 0.9138, 0.4060, 0.5511, Acc_f: 0.0052, Acc_r: 0.7885\n",
      "[train] epoch 8, batch 29, loss 2.472752332687378\n",
      "0.9415, 0.9780, 0.7269, 0.0042, 0.9548, 0.8628, 0.7309, 0.9128, 0.3892, 0.5335, Acc_f: 0.0042, Acc_r: 0.7812\n",
      "[train] epoch 9, batch 29, loss 2.452505350112915\n",
      "0.9404, 0.9774, 0.7149, 0.0024, 0.9540, 0.8645, 0.7258, 0.9113, 0.3711, 0.5197, Acc_f: 0.0024, Acc_r: 0.7755\n",
      "[train] epoch 10, batch 29, loss 2.450673818588257\n",
      "0.9386, 0.9771, 0.6975, 0.0014, 0.9540, 0.8670, 0.7193, 0.9099, 0.3476, 0.5072, Acc_f: 0.0014, Acc_r: 0.7687\n",
      "[train] epoch 11, batch 29, loss 2.3220696449279785\n",
      "0.9364, 0.9765, 0.6753, 0.0007, 0.9532, 0.8700, 0.7127, 0.9049, 0.3373, 0.4909, Acc_f: 0.0007, Acc_r: 0.7619\n",
      "[train] epoch 12, batch 29, loss 2.410547971725464\n",
      "0.9346, 0.9761, 0.6592, 0.0003, 0.9509, 0.8729, 0.7102, 0.9024, 0.3235, 0.4840, Acc_f: 0.0003, Acc_r: 0.7571\n",
      "[train] epoch 13, batch 29, loss 2.3570988178253174\n",
      "0.9318, 0.9765, 0.6402, 0.0000, 0.9505, 0.8763, 0.7046, 0.9024, 0.3114, 0.4777, Acc_f: 0.0000, Acc_r: 0.7524\n",
      "[train] epoch 14, batch 29, loss 2.3329110145568848\n",
      "0.9295, 0.9767, 0.6250, 0.0000, 0.9485, 0.8800, 0.7031, 0.8990, 0.2970, 0.4683, Acc_f: 0.0000, Acc_r: 0.7474\n",
      "[train] epoch 15, batch 29, loss 2.2830333709716797\n",
      "0.9289, 0.9763, 0.6091, 0.0000, 0.9485, 0.8834, 0.7006, 0.8975, 0.2843, 0.4564, Acc_f: 0.0000, Acc_r: 0.7428\n",
      "[train] epoch 16, batch 29, loss 2.314669370651245\n",
      "0.9283, 0.9763, 0.5992, 0.0000, 0.9469, 0.8893, 0.7021, 0.8965, 0.2777, 0.4583, Acc_f: 0.0000, Acc_r: 0.7416\n",
      "[train] epoch 17, batch 29, loss 2.3872992992401123\n",
      "0.9283, 0.9759, 0.5879, 0.0000, 0.9449, 0.8930, 0.7026, 0.8940, 0.2699, 0.4571, Acc_f: 0.0000, Acc_r: 0.7393\n",
      "[train] epoch 18, batch 29, loss 2.307955265045166\n",
      "0.9266, 0.9763, 0.5618, 0.0000, 0.9437, 0.8926, 0.7011, 0.8891, 0.2608, 0.4395, Acc_f: 0.0000, Acc_r: 0.7324\n",
      "[train] epoch 19, batch 29, loss 2.326896905899048\n",
      "0.9249, 0.9755, 0.5418, 0.0000, 0.9390, 0.8972, 0.6990, 0.8871, 0.2548, 0.4376, Acc_f: 0.0000, Acc_r: 0.7285\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "[train] epoch 0, batch 29, loss 2.983672857284546\n",
      "0.8876, 0.9822, 0.8711, 0.3373, 0.9041, 0.9308, 0.8862, 0.7796, 0.7416, 0.8194, Acc_f: 0.3373, Acc_r: 0.8669\n",
      "[train] epoch 1, batch 29, loss 2.5562970638275146\n",
      "0.8050, 0.9749, 0.5780, 0.0423, 0.8300, 0.9169, 0.7663, 0.5706, 0.4373, 0.4834, Acc_f: 0.0423, Acc_r: 0.7069\n",
      "[train] epoch 2, batch 29, loss 2.3814847469329834\n",
      "0.7655, 0.9390, 0.4647, 0.0101, 0.7689, 0.9451, 0.7339, 0.4720, 0.3151, 0.3367, Acc_f: 0.0101, Acc_r: 0.6379\n",
      "[train] epoch 3, batch 29, loss 2.347639322280884\n",
      "0.7414, 0.8763, 0.3835, 0.0031, 0.6694, 0.9669, 0.7309, 0.3992, 0.2620, 0.2339, Acc_f: 0.0031, Acc_r: 0.5848\n",
      "[train] epoch 4, batch 29, loss 2.26318097114563\n",
      "0.7248, 0.8231, 0.3372, 0.0010, 0.6064, 0.9719, 0.7476, 0.3531, 0.2229, 0.1806, Acc_f: 0.0010, Acc_r: 0.5520\n",
      "[train] epoch 5, batch 29, loss 2.3145461082458496\n",
      "0.7150, 0.7798, 0.3006, 0.0010, 0.5628, 0.9753, 0.7668, 0.3259, 0.2072, 0.1411, Acc_f: 0.0010, Acc_r: 0.5305\n",
      "[train] epoch 6, batch 29, loss 2.2436487674713135\n",
      "0.6927, 0.7325, 0.2728, 0.0007, 0.5260, 0.9773, 0.7724, 0.2962, 0.1843, 0.1085, Acc_f: 0.0007, Acc_r: 0.5070\n",
      "[train] epoch 7, batch 29, loss 2.2546029090881348\n",
      "0.6869, 0.7197, 0.2572, 0.0007, 0.5089, 0.9727, 0.7936, 0.2813, 0.1801, 0.1016, Acc_f: 0.0007, Acc_r: 0.5002\n",
      "[train] epoch 8, batch 29, loss 2.2679507732391357\n",
      "0.6697, 0.7019, 0.2405, 0.0003, 0.4883, 0.9732, 0.8093, 0.2684, 0.1723, 0.0890, Acc_f: 0.0003, Acc_r: 0.4903\n",
      "[train] epoch 9, batch 29, loss 2.2535510063171387\n",
      "0.6606, 0.6872, 0.2244, 0.0003, 0.4725, 0.9736, 0.8002, 0.2580, 0.1663, 0.0784, Acc_f: 0.0003, Acc_r: 0.4801\n",
      "[train] epoch 10, batch 29, loss 2.2722296714782715\n",
      "0.6491, 0.6768, 0.2109, 0.0003, 0.4637, 0.9694, 0.8063, 0.2501, 0.1554, 0.0759, Acc_f: 0.0003, Acc_r: 0.4731\n",
      "[train] epoch 11, batch 29, loss 2.2599239349365234\n",
      "0.6422, 0.6841, 0.2061, 0.0003, 0.4649, 0.9614, 0.7977, 0.2506, 0.1440, 0.0746, Acc_f: 0.0003, Acc_r: 0.4695\n",
      "[train] epoch 12, batch 29, loss 2.245126485824585\n",
      "0.6256, 0.6744, 0.1882, 0.0003, 0.4495, 0.9585, 0.7916, 0.2417, 0.1392, 0.0665, Acc_f: 0.0003, Acc_r: 0.4595\n",
      "[train] epoch 13, batch 29, loss 2.248441696166992\n",
      "0.6193, 0.6760, 0.1786, 0.0003, 0.4507, 0.9488, 0.8255, 0.2392, 0.1361, 0.0608, Acc_f: 0.0003, Acc_r: 0.4594\n",
      "[train] epoch 14, batch 29, loss 2.239894151687622\n",
      "0.5963, 0.6503, 0.1694, 0.0003, 0.4293, 0.9564, 0.8093, 0.2328, 0.1277, 0.0527, Acc_f: 0.0003, Acc_r: 0.4471\n",
      "[train] epoch 15, batch 29, loss 2.2379589080810547\n",
      "0.5889, 0.6421, 0.1615, 0.0003, 0.4320, 0.9471, 0.8088, 0.2308, 0.1199, 0.0558, Acc_f: 0.0003, Acc_r: 0.4430\n",
      "[train] epoch 16, batch 29, loss 2.2408852577209473\n",
      "0.5774, 0.6350, 0.1502, 0.0003, 0.4170, 0.9438, 0.7956, 0.2273, 0.1120, 0.0527, Acc_f: 0.0003, Acc_r: 0.4346\n",
      "[train] epoch 17, batch 29, loss 2.2440414428710938\n",
      "0.5740, 0.6421, 0.1417, 0.0000, 0.4146, 0.9354, 0.8108, 0.2189, 0.1018, 0.0508, Acc_f: 0.0000, Acc_r: 0.4322\n",
      "[train] epoch 18, batch 29, loss 2.236920118331909\n",
      "0.5562, 0.6331, 0.1335, 0.0000, 0.3967, 0.9337, 0.8068, 0.2070, 0.0952, 0.0476, Acc_f: 0.0000, Acc_r: 0.4233\n",
      "[train] epoch 19, batch 29, loss 2.23461651802063\n",
      "0.5453, 0.6237, 0.1261, 0.0000, 0.3849, 0.9325, 0.8189, 0.2026, 0.0904, 0.0439, Acc_f: 0.0000, Acc_r: 0.4187\n"
     ]
    }
   ],
   "source": [
    "Acc_r, Acc_f = np.zeros(3), np.zeros(3)\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(20):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        Acc_r[i], Acc_f[i] = test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label Acc_f: 1.17 \\pm 0.16\n",
      "Random label Acc_r: 75.87 \\pm 8.67\n"
     ]
    }
   ],
   "source": [
    "Acc_f = 100*np.array([0.0139, 0.0111, 0.0101])\n",
    "Acc_r = 100*np.array([0.8372, 0.8010, 0.6379])\n",
    "print(f'Random label Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 62/576\n",
      "Layer 3 : 734/1152\n",
      "Layer 4 : 1835/2304\n",
      "Layer 5 : 844/2304\n",
      "Layer 6 : 896/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 29/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 5/27\n",
      "Layer 2 : 60/576\n",
      "Layer 3 : 739/1152\n",
      "Layer 4 : 1832/2304\n",
      "Layer 5 : 843/2304\n",
      "Layer 6 : 899/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 27/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "Threshold:  0.99\n",
      "----------------------------------------\n",
      "Gradient Constraints Summary\n",
      "----------------------------------------\n",
      "Layer 1 : 6/27\n",
      "Layer 2 : 66/576\n",
      "Layer 3 : 761/1152\n",
      "Layer 4 : 1854/2304\n",
      "Layer 5 : 847/2304\n",
      "Layer 6 : 906/4608\n",
      "Layer 7 : 0/4608\n",
      "Layer 8 : 0/4608\n",
      "Layer 9 : 33/512\n",
      "Layer 10 : 9/4096\n",
      "Layer 11 : 8/4096\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Proj_mat_lst =[]\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    \n",
    "    feature_list = []\n",
    "    merged_feat_mat = []\n",
    "    for batch, (x, y) in enumerate(remain_loader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        mat_list = get_representation_matrix(model, x, batch_list=[256]*30)\n",
    "        break\n",
    "    threshold = 0.99\n",
    "    merged_feat_mat = update_GPM(mat_list, threshold, merged_feat_mat)\n",
    "    proj_mat = [torch.Tensor(np.dot(layer_basis, layer_basis.transpose())) for layer_basis in merged_feat_mat]\n",
    "    Proj_mat_lst.append(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 29, loss 2.62646484375\n",
      "0.9667, 0.9694, 0.9689, 0.0906, 0.9734, 0.9560, 0.9509, 0.9366, 0.9289, 0.9310, Acc_f: 0.0906, Acc_r: 0.9535\n",
      "[train] epoch 1, batch 29, loss 2.4480502605438232\n",
      "0.9662, 0.9686, 0.9679, 0.0160, 0.9738, 0.9585, 0.9555, 0.9341, 0.9247, 0.9191, Acc_f: 0.0160, Acc_r: 0.9521\n",
      "[train] epoch 2, batch 29, loss 2.3905811309814453\n",
      "0.9662, 0.9674, 0.9689, 0.0045, 0.9734, 0.9581, 0.9560, 0.9326, 0.9241, 0.9047, Acc_f: 0.0045, Acc_r: 0.9502\n",
      "[train] epoch 3, batch 29, loss 2.260897636413574\n",
      "0.9667, 0.9667, 0.9687, 0.0010, 0.9742, 0.9597, 0.9560, 0.9312, 0.9223, 0.8878, Acc_f: 0.0010, Acc_r: 0.9481\n",
      "[train] epoch 4, batch 29, loss 2.329035520553589\n",
      "0.9667, 0.9653, 0.9646, 0.0003, 0.9738, 0.9585, 0.9550, 0.9247, 0.9217, 0.8683, Acc_f: 0.0003, Acc_r: 0.9443\n",
      "[train] epoch 5, batch 29, loss 2.2898318767547607\n",
      "0.9667, 0.9647, 0.9614, 0.0000, 0.9734, 0.9610, 0.9530, 0.9193, 0.9199, 0.8339, Acc_f: 0.0000, Acc_r: 0.9393\n",
      "[train] epoch 6, batch 29, loss 2.2416954040527344\n",
      "0.9685, 0.9645, 0.9578, 0.0000, 0.9734, 0.9622, 0.9519, 0.9158, 0.9199, 0.8182, Acc_f: 0.0000, Acc_r: 0.9369\n",
      "[train] epoch 7, batch 29, loss 2.2717995643615723\n",
      "0.9685, 0.9641, 0.9496, 0.0000, 0.9719, 0.9597, 0.9514, 0.9089, 0.9205, 0.7925, Acc_f: 0.0000, Acc_r: 0.9319\n",
      "[train] epoch 8, batch 29, loss 2.2865681648254395\n",
      "0.9690, 0.9639, 0.9455, 0.0000, 0.9727, 0.9568, 0.9504, 0.9069, 0.9193, 0.7843, Acc_f: 0.0000, Acc_r: 0.9299\n",
      "[train] epoch 9, batch 29, loss 2.250086784362793\n",
      "0.9690, 0.9635, 0.9395, 0.0000, 0.9711, 0.9614, 0.9494, 0.9004, 0.9211, 0.7586, Acc_f: 0.0000, Acc_r: 0.9260\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n",
      "[train] epoch 0, batch 29, loss 2.761357069015503\n",
      "0.9696, 0.9712, 0.9696, 0.1985, 0.9738, 0.9635, 0.9428, 0.9470, 0.9108, 0.9498, Acc_f: 0.1985, Acc_r: 0.9554\n",
      "[train] epoch 1, batch 29, loss 2.504655122756958\n",
      "0.9696, 0.9710, 0.9665, 0.0430, 0.9738, 0.9627, 0.9378, 0.9485, 0.8873, 0.9442, Acc_f: 0.0430, Acc_r: 0.9513\n",
      "[train] epoch 2, batch 29, loss 2.4416675567626953\n",
      "0.9690, 0.9716, 0.9636, 0.0139, 0.9738, 0.9618, 0.9353, 0.9475, 0.8819, 0.9448, Acc_f: 0.0139, Acc_r: 0.9499\n",
      "[train] epoch 3, batch 29, loss 2.3743810653686523\n",
      "0.9685, 0.9720, 0.9629, 0.0056, 0.9734, 0.9610, 0.9347, 0.9470, 0.8783, 0.9417, Acc_f: 0.0056, Acc_r: 0.9488\n",
      "[train] epoch 4, batch 29, loss 2.306718111038208\n",
      "0.9685, 0.9725, 0.9610, 0.0024, 0.9734, 0.9622, 0.9342, 0.9470, 0.8759, 0.9436, Acc_f: 0.0024, Acc_r: 0.9487\n",
      "[train] epoch 5, batch 29, loss 2.3315703868865967\n",
      "0.9685, 0.9718, 0.9607, 0.0014, 0.9738, 0.9631, 0.9373, 0.9475, 0.8741, 0.9398, Acc_f: 0.0014, Acc_r: 0.9485\n",
      "[train] epoch 6, batch 29, loss 2.3039095401763916\n",
      "0.9690, 0.9718, 0.9583, 0.0010, 0.9738, 0.9635, 0.9383, 0.9490, 0.8741, 0.9404, Acc_f: 0.0010, Acc_r: 0.9487\n",
      "[train] epoch 7, batch 29, loss 2.3474819660186768\n",
      "0.9690, 0.9714, 0.9564, 0.0003, 0.9738, 0.9635, 0.9398, 0.9495, 0.8705, 0.9354, Acc_f: 0.0003, Acc_r: 0.9477\n",
      "[train] epoch 8, batch 29, loss 2.3099279403686523\n",
      "0.9690, 0.9698, 0.9569, 0.0003, 0.9746, 0.9664, 0.9408, 0.9485, 0.8608, 0.9348, Acc_f: 0.0003, Acc_r: 0.9469\n",
      "[train] epoch 9, batch 29, loss 2.279653787612915\n",
      "0.9690, 0.9690, 0.9561, 0.0000, 0.9750, 0.9664, 0.9433, 0.9510, 0.8590, 0.9285, Acc_f: 0.0000, Acc_r: 0.9464\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "[train] epoch 0, batch 29, loss 2.4379546642303467\n",
      "0.9530, 0.9769, 0.9542, 0.0566, 0.9663, 0.9606, 0.9474, 0.8975, 0.8777, 0.8934, Acc_f: 0.0566, Acc_r: 0.9363\n",
      "[train] epoch 1, batch 29, loss 2.3061037063598633\n",
      "0.9507, 0.9749, 0.9390, 0.0073, 0.9620, 0.9643, 0.9519, 0.8920, 0.8645, 0.8044, Acc_f: 0.0073, Acc_r: 0.9226\n",
      "[train] epoch 2, batch 29, loss 2.284473180770874\n",
      "0.9484, 0.9739, 0.9260, 0.0003, 0.9608, 0.9656, 0.9504, 0.8940, 0.8500, 0.7624, Acc_f: 0.0003, Acc_r: 0.9146\n",
      "[train] epoch 3, batch 29, loss 2.2845845222473145\n",
      "0.9490, 0.9733, 0.9142, 0.0000, 0.9588, 0.9643, 0.9535, 0.8900, 0.8386, 0.7122, Acc_f: 0.0000, Acc_r: 0.9060\n",
      "[train] epoch 4, batch 29, loss 2.256801128387451\n",
      "0.9455, 0.9729, 0.8838, 0.0000, 0.9592, 0.9627, 0.9530, 0.8891, 0.8205, 0.6583, Acc_f: 0.0000, Acc_r: 0.8939\n",
      "[train] epoch 5, batch 29, loss 2.2170908451080322\n",
      "0.9450, 0.9718, 0.8679, 0.0000, 0.9604, 0.9593, 0.9514, 0.8791, 0.7910, 0.6351, Acc_f: 0.0000, Acc_r: 0.8846\n",
      "[train] epoch 6, batch 29, loss 2.223304033279419\n",
      "0.9432, 0.9714, 0.8479, 0.0000, 0.9548, 0.9589, 0.9535, 0.8707, 0.7964, 0.6050, Acc_f: 0.0000, Acc_r: 0.8780\n",
      "[train] epoch 7, batch 29, loss 2.250502586364746\n",
      "0.9432, 0.9698, 0.8161, 0.0000, 0.9548, 0.9610, 0.9530, 0.8514, 0.7873, 0.5724, Acc_f: 0.0000, Acc_r: 0.8677\n",
      "[train] epoch 8, batch 29, loss 2.2329626083374023\n",
      "0.9427, 0.9696, 0.8139, 0.0000, 0.9489, 0.9589, 0.9525, 0.8687, 0.7916, 0.5712, Acc_f: 0.0000, Acc_r: 0.8687\n",
      "[train] epoch 9, batch 29, loss 2.2187235355377197\n",
      "0.9444, 0.9698, 0.8033, 0.0000, 0.9465, 0.9492, 0.9580, 0.8559, 0.7976, 0.5498, Acc_f: 0.0000, Acc_r: 0.8638\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random label + Subspace Acc_f: 2.88 \\pm 1.97\n",
      "Random label + Subspace Acc_r: 94.61 \\pm 0.70\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9521, 0.9499, 0.9363])\n",
    "Acc_f = 100*np.array([0.0160, 0.0139, 0.0566])\n",
    "\n",
    "print(f'Random label + Subspace Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Random label + Subspace Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 29, loss 0.5311841368675232\n",
      "0.9673, 0.9739, 0.9788, 0.0000, 0.9620, 0.9673, 0.9535, 0.9386, 0.9464, 0.9335, Acc_f: 0.0000, Acc_r: 0.9579\n",
      "[train] epoch 1, batch 29, loss 0.47979700565338135\n",
      "0.9644, 0.9692, 0.9773, 0.0000, 0.9663, 0.9706, 0.9525, 0.9445, 0.9530, 0.9304, Acc_f: 0.0000, Acc_r: 0.9587\n",
      "[train] epoch 2, batch 29, loss 0.2808799147605896\n",
      "0.9667, 0.9720, 0.9761, 0.0000, 0.9671, 0.9631, 0.9595, 0.9416, 0.9524, 0.9285, Acc_f: 0.0000, Acc_r: 0.9586\n",
      "[train] epoch 3, batch 29, loss 0.2709907293319702\n",
      "0.9667, 0.9714, 0.9769, 0.0000, 0.9631, 0.9669, 0.9509, 0.9445, 0.9584, 0.9335, Acc_f: 0.0000, Acc_r: 0.9592\n",
      "[train] epoch 4, batch 29, loss 0.523321807384491\n",
      "0.9690, 0.9741, 0.9771, 0.0000, 0.9627, 0.9555, 0.9540, 0.9450, 0.9602, 0.9348, Acc_f: 0.0000, Acc_r: 0.9592\n",
      "[train] epoch 5, batch 29, loss 0.3216308653354645\n",
      "0.9656, 0.9735, 0.9696, 0.0000, 0.9655, 0.9635, 0.9550, 0.9460, 0.9608, 0.9342, Acc_f: 0.0000, Acc_r: 0.9593\n",
      "[train] epoch 6, batch 29, loss 0.3125930428504944\n",
      "0.9679, 0.9684, 0.9785, 0.0000, 0.9667, 0.9643, 0.9519, 0.9475, 0.9584, 0.9285, Acc_f: 0.0000, Acc_r: 0.9591\n",
      "[train] epoch 7, batch 29, loss 0.30151599645614624\n",
      "0.9673, 0.9722, 0.9752, 0.0000, 0.9667, 0.9614, 0.9535, 0.9450, 0.9596, 0.9298, Acc_f: 0.0000, Acc_r: 0.9590\n",
      "[train] epoch 8, batch 29, loss 0.24826274812221527\n",
      "0.9662, 0.9702, 0.9730, 0.0000, 0.9647, 0.9660, 0.9530, 0.9490, 0.9602, 0.9310, Acc_f: 0.0000, Acc_r: 0.9593\n",
      "[train] epoch 9, batch 29, loss 0.45615971088409424\n",
      "0.9702, 0.9751, 0.9675, 0.0000, 0.9667, 0.9568, 0.9570, 0.9485, 0.9614, 0.9335, Acc_f: 0.0000, Acc_r: 0.9596\n",
      "[train] epoch 10, batch 29, loss 0.41867679357528687\n",
      "0.9685, 0.9672, 0.9759, 0.0000, 0.9671, 0.9610, 0.9525, 0.9500, 0.9614, 0.9285, Acc_f: 0.0000, Acc_r: 0.9591\n",
      "[train] epoch 11, batch 29, loss 0.23632648587226868\n",
      "0.9667, 0.9696, 0.9737, 0.0000, 0.9683, 0.9639, 0.9545, 0.9470, 0.9602, 0.9335, Acc_f: 0.0000, Acc_r: 0.9597\n",
      "[train] epoch 12, batch 29, loss 0.27831682562828064\n",
      "0.9679, 0.9712, 0.9752, 0.0000, 0.9663, 0.9627, 0.9514, 0.9460, 0.9596, 0.9310, Acc_f: 0.0000, Acc_r: 0.9590\n",
      "[train] epoch 13, batch 29, loss 0.22281624376773834\n",
      "0.9667, 0.9733, 0.9706, 0.0000, 0.9675, 0.9618, 0.9555, 0.9470, 0.9602, 0.9317, Acc_f: 0.0000, Acc_r: 0.9594\n",
      "[train] epoch 14, batch 29, loss 0.16230450570583344\n",
      "0.9656, 0.9698, 0.9771, 0.0000, 0.9647, 0.9610, 0.9550, 0.9475, 0.9614, 0.9285, Acc_f: 0.0000, Acc_r: 0.9590\n",
      "[train] epoch 15, batch 29, loss 0.18292635679244995\n",
      "0.9685, 0.9723, 0.9720, 0.0000, 0.9663, 0.9610, 0.9514, 0.9485, 0.9602, 0.9335, Acc_f: 0.0000, Acc_r: 0.9593\n",
      "[train] epoch 16, batch 29, loss 0.23240701854228973\n",
      "0.9644, 0.9710, 0.9766, 0.0000, 0.9659, 0.9618, 0.9525, 0.9465, 0.9608, 0.9310, Acc_f: 0.0000, Acc_r: 0.9590\n",
      "[train] epoch 17, batch 29, loss 0.28260207176208496\n",
      "0.9644, 0.9716, 0.9766, 0.0000, 0.9667, 0.9618, 0.9514, 0.9440, 0.9572, 0.9342, Acc_f: 0.0000, Acc_r: 0.9587\n",
      "[train] epoch 18, batch 29, loss 0.2479572594165802\n",
      "0.9673, 0.9718, 0.9701, 0.0000, 0.9663, 0.9631, 0.9565, 0.9495, 0.9548, 0.9298, Acc_f: 0.0000, Acc_r: 0.9588\n",
      "[train] epoch 19, batch 29, loss 0.2618357241153717\n",
      "0.9673, 0.9700, 0.9757, 0.0000, 0.9663, 0.9606, 0.9565, 0.9475, 0.9584, 0.9248, Acc_f: 0.0000, Acc_r: 0.9586\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n",
      "[train] epoch 0, batch 29, loss 0.7777236700057983\n",
      "0.9610, 0.9627, 0.9800, 0.0000, 0.9608, 0.9803, 0.9332, 0.9480, 0.9602, 0.9461, Acc_f: 0.0000, Acc_r: 0.9591\n",
      "[train] epoch 1, batch 29, loss 0.3342066705226898\n",
      "0.9656, 0.9722, 0.9752, 0.0000, 0.9596, 0.9778, 0.9368, 0.9371, 0.9627, 0.9530, Acc_f: 0.0000, Acc_r: 0.9600\n",
      "[train] epoch 2, batch 29, loss 0.31067532300949097\n",
      "0.9685, 0.9700, 0.9781, 0.0000, 0.9612, 0.9765, 0.9378, 0.9440, 0.9633, 0.9486, Acc_f: 0.0000, Acc_r: 0.9609\n",
      "[train] epoch 3, batch 29, loss 0.2801664471626282\n",
      "0.9702, 0.9743, 0.9781, 0.0000, 0.9608, 0.9740, 0.9398, 0.9376, 0.9627, 0.9473, Acc_f: 0.0000, Acc_r: 0.9605\n",
      "[train] epoch 4, batch 29, loss 0.23501308262348175\n",
      "0.9696, 0.9708, 0.9769, 0.0000, 0.9651, 0.9744, 0.9388, 0.9386, 0.9627, 0.9480, Acc_f: 0.0000, Acc_r: 0.9605\n",
      "[train] epoch 5, batch 29, loss 0.27968916296958923\n",
      "0.9708, 0.9720, 0.9776, 0.0000, 0.9623, 0.9736, 0.9393, 0.9411, 0.9620, 0.9467, Acc_f: 0.0000, Acc_r: 0.9606\n",
      "[train] epoch 6, batch 29, loss 0.21485860645771027\n",
      "0.9685, 0.9702, 0.9783, 0.0000, 0.9647, 0.9744, 0.9398, 0.9425, 0.9620, 0.9511, Acc_f: 0.0000, Acc_r: 0.9613\n",
      "[train] epoch 7, batch 29, loss 0.22604113817214966\n",
      "0.9679, 0.9696, 0.9788, 0.0000, 0.9635, 0.9736, 0.9413, 0.9480, 0.9578, 0.9511, Acc_f: 0.0000, Acc_r: 0.9613\n",
      "[train] epoch 8, batch 29, loss 0.20842468738555908\n",
      "0.9650, 0.9700, 0.9759, 0.0000, 0.9651, 0.9744, 0.9418, 0.9430, 0.9639, 0.9505, Acc_f: 0.0000, Acc_r: 0.9611\n",
      "[train] epoch 9, batch 29, loss 0.2521510720252991\n",
      "0.9656, 0.9706, 0.9793, 0.0000, 0.9639, 0.9748, 0.9408, 0.9411, 0.9639, 0.9505, Acc_f: 0.0000, Acc_r: 0.9612\n",
      "[train] epoch 10, batch 29, loss 0.4781794846057892\n",
      "0.9685, 0.9678, 0.9778, 0.0000, 0.9643, 0.9765, 0.9373, 0.9460, 0.9584, 0.9498, Acc_f: 0.0000, Acc_r: 0.9607\n",
      "[train] epoch 11, batch 29, loss 0.23755759000778198\n",
      "0.9667, 0.9708, 0.9718, 0.0000, 0.9639, 0.9744, 0.9403, 0.9421, 0.9627, 0.9542, Acc_f: 0.0000, Acc_r: 0.9608\n",
      "[train] epoch 12, batch 29, loss 0.19101977348327637\n",
      "0.9679, 0.9714, 0.9788, 0.0000, 0.9635, 0.9719, 0.9428, 0.9346, 0.9590, 0.9486, Acc_f: 0.0000, Acc_r: 0.9598\n",
      "[train] epoch 13, batch 29, loss 0.23260273039340973\n",
      "0.9667, 0.9696, 0.9781, 0.0000, 0.9608, 0.9748, 0.9433, 0.9440, 0.9578, 0.9480, Acc_f: 0.0000, Acc_r: 0.9604\n",
      "[train] epoch 14, batch 29, loss 0.2373453825712204\n",
      "0.9662, 0.9716, 0.9807, 0.0000, 0.9600, 0.9727, 0.9459, 0.9366, 0.9548, 0.9492, Acc_f: 0.0000, Acc_r: 0.9597\n",
      "[train] epoch 15, batch 29, loss 0.22330884635448456\n",
      "0.9662, 0.9712, 0.9737, 0.0000, 0.9627, 0.9727, 0.9464, 0.9396, 0.9572, 0.9505, Acc_f: 0.0000, Acc_r: 0.9600\n",
      "[train] epoch 16, batch 29, loss 0.17463262379169464\n",
      "0.9667, 0.9718, 0.9742, 0.0000, 0.9627, 0.9719, 0.9474, 0.9386, 0.9584, 0.9505, Acc_f: 0.0000, Acc_r: 0.9602\n",
      "[train] epoch 17, batch 29, loss 0.21185824275016785\n",
      "0.9656, 0.9708, 0.9754, 0.0000, 0.9639, 0.9736, 0.9469, 0.9406, 0.9572, 0.9473, Acc_f: 0.0000, Acc_r: 0.9601\n",
      "[train] epoch 18, batch 29, loss 0.1700359433889389\n",
      "0.9662, 0.9716, 0.9742, 0.0000, 0.9612, 0.9753, 0.9484, 0.9416, 0.9560, 0.9492, Acc_f: 0.0000, Acc_r: 0.9604\n",
      "[train] epoch 19, batch 29, loss 0.17365038394927979\n",
      "0.9656, 0.9710, 0.9776, 0.0000, 0.9608, 0.9723, 0.9449, 0.9406, 0.9572, 0.9524, Acc_f: 0.0000, Acc_r: 0.9602\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "[train] epoch 0, batch 29, loss 0.3601994216442108\n",
      "0.9352, 0.9798, 0.9785, 0.0000, 0.9520, 0.9539, 0.9403, 0.9094, 0.9639, 0.9260, Acc_f: 0.0000, Acc_r: 0.9488\n",
      "[train] epoch 1, batch 29, loss 0.38558363914489746\n",
      "0.9369, 0.9827, 0.9704, 0.0000, 0.9481, 0.9643, 0.9408, 0.9089, 0.9645, 0.9204, Acc_f: 0.0000, Acc_r: 0.9486\n",
      "[train] epoch 2, batch 29, loss 0.3393436074256897\n",
      "0.9329, 0.9735, 0.9742, 0.0000, 0.9564, 0.9648, 0.9393, 0.9183, 0.9639, 0.9266, Acc_f: 0.0000, Acc_r: 0.9500\n",
      "[train] epoch 3, batch 29, loss 0.3021070957183838\n",
      "0.9404, 0.9808, 0.9781, 0.0000, 0.9512, 0.9505, 0.9428, 0.9039, 0.9602, 0.9166, Acc_f: 0.0000, Acc_r: 0.9472\n",
      "[train] epoch 4, batch 29, loss 0.423005610704422\n",
      "0.9364, 0.9759, 0.9754, 0.0000, 0.9552, 0.9589, 0.9423, 0.9173, 0.9602, 0.9091, Acc_f: 0.0000, Acc_r: 0.9479\n",
      "[train] epoch 5, batch 29, loss 0.22689588367938995\n",
      "0.9369, 0.9806, 0.9708, 0.0000, 0.9512, 0.9652, 0.9413, 0.9118, 0.9602, 0.9292, Acc_f: 0.0000, Acc_r: 0.9497\n",
      "[train] epoch 6, batch 29, loss 0.2393299639225006\n",
      "0.9415, 0.9776, 0.9696, 0.0000, 0.9556, 0.9614, 0.9398, 0.9203, 0.9639, 0.9367, Acc_f: 0.0000, Acc_r: 0.9518\n",
      "[train] epoch 7, batch 29, loss 0.4045551121234894\n",
      "0.9392, 0.9802, 0.9667, 0.0000, 0.9560, 0.9606, 0.9403, 0.9237, 0.9651, 0.9361, Acc_f: 0.0000, Acc_r: 0.9520\n",
      "[train] epoch 8, batch 29, loss 0.25350475311279297\n",
      "0.9352, 0.9753, 0.9735, 0.0000, 0.9584, 0.9581, 0.9418, 0.9208, 0.9620, 0.9254, Acc_f: 0.0000, Acc_r: 0.9500\n",
      "[train] epoch 9, batch 29, loss 0.2037983536720276\n",
      "0.9346, 0.9771, 0.9679, 0.0000, 0.9576, 0.9614, 0.9403, 0.9277, 0.9669, 0.9342, Acc_f: 0.0000, Acc_r: 0.9520\n",
      "[train] epoch 10, batch 29, loss 0.20791681110858917\n",
      "0.9392, 0.9796, 0.9708, 0.0000, 0.9548, 0.9606, 0.9418, 0.9153, 0.9645, 0.9197, Acc_f: 0.0000, Acc_r: 0.9496\n",
      "[train] epoch 11, batch 29, loss 0.20720221102237701\n",
      "0.9409, 0.9759, 0.9728, 0.0000, 0.9556, 0.9618, 0.9408, 0.9321, 0.9645, 0.9191, Acc_f: 0.0000, Acc_r: 0.9515\n",
      "[train] epoch 12, batch 29, loss 0.19280309975147247\n",
      "0.9450, 0.9780, 0.9701, 0.0000, 0.9560, 0.9560, 0.9423, 0.9193, 0.9663, 0.9266, Acc_f: 0.0000, Acc_r: 0.9511\n",
      "[train] epoch 13, batch 29, loss 0.21704453229904175\n",
      "0.9415, 0.9796, 0.9704, 0.0000, 0.9592, 0.9518, 0.9439, 0.9173, 0.9602, 0.9223, Acc_f: 0.0000, Acc_r: 0.9496\n",
      "[train] epoch 14, batch 29, loss 0.17444710433483124\n",
      "0.9404, 0.9722, 0.9740, 0.0000, 0.9572, 0.9560, 0.9393, 0.9316, 0.9614, 0.9310, Acc_f: 0.0000, Acc_r: 0.9515\n",
      "[train] epoch 15, batch 29, loss 0.227824866771698\n",
      "0.9427, 0.9784, 0.9718, 0.0000, 0.9556, 0.9505, 0.9464, 0.9232, 0.9590, 0.9367, Acc_f: 0.0000, Acc_r: 0.9516\n",
      "[train] epoch 16, batch 29, loss 0.20928780734539032\n",
      "0.9375, 0.9786, 0.9684, 0.0000, 0.9509, 0.9589, 0.9459, 0.9227, 0.9645, 0.9292, Acc_f: 0.0000, Acc_r: 0.9507\n",
      "[train] epoch 17, batch 29, loss 0.16583974659442902\n",
      "0.9369, 0.9769, 0.9679, 0.0000, 0.9552, 0.9564, 0.9439, 0.9277, 0.9645, 0.9361, Acc_f: 0.0000, Acc_r: 0.9517\n",
      "[train] epoch 18, batch 29, loss 0.16978687047958374\n",
      "0.9398, 0.9776, 0.9677, 0.0000, 0.9560, 0.9614, 0.9433, 0.9227, 0.9645, 0.9279, Acc_f: 0.0000, Acc_r: 0.9512\n",
      "[train] epoch 19, batch 29, loss 0.1715926229953766\n",
      "0.9415, 0.9782, 0.9701, 0.0000, 0.9544, 0.9639, 0.9403, 0.9198, 0.9614, 0.9323, Acc_f: 0.0000, Acc_r: 0.9513\n"
     ]
    }
   ],
   "source": [
    "def get_2nd_score(model, x, y):\n",
    "    indices = torch.topk(model(x), k=2, dim=1).indices\n",
    "    top1_matches = indices[:, 0] == y\n",
    "    selected_labels = torch.where(top1_matches, indices[:, 1], indices[:, 0])\n",
    "    return selected_labels\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.03)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    model.eval()\n",
    "    for ep in range(10):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = get_2nd_score(model, x, y.cuda())\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            kk = 0 \n",
    "            for k, (m,params) in enumerate(sgd_mr_model.named_parameters()):\n",
    "                if len(params.size())!=1:\n",
    "                    sz =  params.grad.data.size(0)\n",
    "                    params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
    "                                            Proj_mat_lst[i][kk].cuda()).view(params.size())\n",
    "                    kk +=1\n",
    "                elif len(params.size())==1:\n",
    "                    params.grad.data.fill_(0)\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_r = 100*np.array([0.9597, 0.9613, 0.9520])\n",
    "Acc_f = 100*np.array([0.0000, 0.0000, 0.0000])\n",
    "print(f'UNSC Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'UNSC Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================================================================================================\n",
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 29, loss 2.1821048259735107\n",
      "0.9157, 0.9831, 0.9489, 0.0219, 0.8692, 0.9220, 0.7886, 0.6612, 0.7837, 0.3411, Acc_f: 0.0219, Acc_r: 0.8015\n",
      "[train] epoch 1, batch 29, loss 2.0913467407226562\n",
      "0.8939, 0.9886, 0.9197, 0.0330, 0.8292, 0.9249, 0.7845, 0.5656, 0.8018, 0.1824, Acc_f: 0.0330, Acc_r: 0.7656\n",
      "[train] epoch 2, batch 29, loss 2.014497756958008\n",
      "0.8647, 0.9943, 0.8318, 0.0402, 0.7634, 0.8813, 0.7527, 0.4482, 0.8018, 0.1034, Acc_f: 0.0402, Acc_r: 0.7157\n",
      "[train] epoch 3, batch 29, loss 1.945489764213562\n",
      "0.8544, 0.9953, 0.8110, 0.0854, 0.7380, 0.8616, 0.7405, 0.4106, 0.8060, 0.0740, Acc_f: 0.0854, Acc_r: 0.6990\n",
      "[train] epoch 4, batch 29, loss 2.079373598098755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8400, 0.9953, 0.8286, 0.1249, 0.7198, 0.8779, 0.7157, 0.3829, 0.8084, 0.0589, Acc_f: 0.1249, Acc_r: 0.6920\n",
      "[train] epoch 5, batch 29, loss 1.9520225524902344\n",
      "0.8389, 0.9953, 0.8467, 0.1634, 0.7122, 0.8624, 0.7051, 0.3824, 0.8151, 0.0564, Acc_f: 0.1634, Acc_r: 0.6905\n",
      "[train] epoch 6, batch 29, loss 1.9265741109848022\n",
      "0.8406, 0.9953, 0.8501, 0.1728, 0.7095, 0.8523, 0.7076, 0.3834, 0.8343, 0.0552, Acc_f: 0.1728, Acc_r: 0.6920\n",
      "[train] epoch 7, batch 29, loss 1.9234846830368042\n",
      "0.8303, 0.9953, 0.8450, 0.1308, 0.6770, 0.8440, 0.6930, 0.3655, 0.8271, 0.0495, Acc_f: 0.1308, Acc_r: 0.6807\n",
      "[train] epoch 8, batch 29, loss 1.900150179862976\n",
      "0.8446, 0.9947, 0.8670, 0.2474, 0.6996, 0.8561, 0.7233, 0.3868, 0.8488, 0.0596, Acc_f: 0.2474, Acc_r: 0.6978\n",
      "[train] epoch 9, batch 29, loss 1.9177441596984863\n",
      "0.8498, 0.9947, 0.8785, 0.2578, 0.7079, 0.8708, 0.7360, 0.4156, 0.8620, 0.0633, Acc_f: 0.2578, Acc_r: 0.7087\n",
      "[train] epoch 10, batch 29, loss 1.8469661474227905\n",
      "0.8498, 0.9949, 0.8766, 0.2217, 0.7071, 0.8586, 0.7360, 0.4185, 0.8639, 0.0633, Acc_f: 0.2217, Acc_r: 0.7076\n",
      "[train] epoch 11, batch 29, loss 1.8851394653320312\n",
      "0.8503, 0.9943, 0.9024, 0.2006, 0.7051, 0.8591, 0.7289, 0.4205, 0.8675, 0.0621, Acc_f: 0.2006, Acc_r: 0.7100\n",
      "[train] epoch 12, batch 29, loss 1.9425004720687866\n",
      "0.8578, 0.9945, 0.9029, 0.2137, 0.7222, 0.8544, 0.7481, 0.4428, 0.8711, 0.0727, Acc_f: 0.2137, Acc_r: 0.7185\n",
      "[train] epoch 13, batch 29, loss 1.911004662513733\n",
      "0.8607, 0.9943, 0.9113, 0.2054, 0.7344, 0.8645, 0.7486, 0.4611, 0.8717, 0.0790, Acc_f: 0.2054, Acc_r: 0.7251\n",
      "[train] epoch 14, batch 29, loss 1.8558659553527832\n",
      "0.8618, 0.9949, 0.8968, 0.1964, 0.7344, 0.8507, 0.7577, 0.4656, 0.8789, 0.0784, Acc_f: 0.1964, Acc_r: 0.7244\n",
      "====================================================================================================================================================================================\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n",
      "[train] epoch 0, batch 29, loss 2.304494857788086\n",
      "0.9513, 0.9869, 0.9241, 0.1287, 0.9505, 0.9773, 0.8816, 0.8821, 0.7747, 0.8000, Acc_f: 0.1287, Acc_r: 0.9032\n",
      "[train] epoch 1, batch 29, loss 2.118175745010376\n",
      "0.9409, 0.9904, 0.8903, 0.0760, 0.9390, 0.9790, 0.8639, 0.8227, 0.7753, 0.6972, Acc_f: 0.0760, Acc_r: 0.8776\n",
      "[train] epoch 2, batch 29, loss 2.0320210456848145\n",
      "0.9421, 0.9914, 0.8723, 0.0680, 0.9199, 0.9790, 0.8665, 0.7845, 0.8084, 0.6652, Acc_f: 0.0680, Acc_r: 0.8699\n",
      "[train] epoch 3, batch 29, loss 2.0403687953948975\n",
      "0.9455, 0.9929, 0.8539, 0.0659, 0.9120, 0.9719, 0.8690, 0.7420, 0.8392, 0.6382, Acc_f: 0.0659, Acc_r: 0.8627\n",
      "[train] epoch 4, batch 29, loss 2.032649278640747\n",
      "0.9530, 0.9925, 0.8515, 0.0933, 0.9100, 0.9685, 0.8720, 0.7231, 0.8687, 0.6621, Acc_f: 0.0933, Acc_r: 0.8668\n",
      "[train] epoch 5, batch 29, loss 2.148693323135376\n",
      "0.9553, 0.9929, 0.8578, 0.0944, 0.9080, 0.9648, 0.8705, 0.6939, 0.8898, 0.6734, Acc_f: 0.0944, Acc_r: 0.8674\n",
      "[train] epoch 6, batch 29, loss 2.0193545818328857\n",
      "0.9587, 0.9927, 0.8653, 0.1010, 0.9037, 0.9627, 0.8715, 0.6731, 0.9030, 0.6928, Acc_f: 0.1010, Acc_r: 0.8693\n",
      "[train] epoch 7, batch 29, loss 1.9866071939468384\n",
      "0.9633, 0.9920, 0.8790, 0.1225, 0.9045, 0.9568, 0.8796, 0.6696, 0.9127, 0.7172, Acc_f: 0.1225, Acc_r: 0.8750\n",
      "[train] epoch 8, batch 29, loss 1.943149209022522\n",
      "0.9639, 0.9918, 0.8831, 0.1343, 0.9073, 0.9581, 0.8781, 0.6662, 0.9163, 0.7317, Acc_f: 0.1343, Acc_r: 0.8774\n",
      "[train] epoch 9, batch 29, loss 2.0542848110198975\n",
      "0.9644, 0.9918, 0.8802, 0.1291, 0.9065, 0.9513, 0.8781, 0.6578, 0.9199, 0.7411, Acc_f: 0.1291, Acc_r: 0.8768\n",
      "[train] epoch 10, batch 29, loss 1.9998581409454346\n",
      "0.9644, 0.9914, 0.8824, 0.1058, 0.9080, 0.9497, 0.8660, 0.6508, 0.9193, 0.7285, Acc_f: 0.1058, Acc_r: 0.8734\n",
      "[train] epoch 11, batch 29, loss 1.9372013807296753\n",
      "0.9644, 0.9912, 0.8935, 0.1266, 0.9136, 0.9534, 0.8741, 0.6578, 0.9217, 0.7455, Acc_f: 0.1266, Acc_r: 0.8795\n",
      "[train] epoch 12, batch 29, loss 2.031440258026123\n",
      "0.9639, 0.9916, 0.8930, 0.1332, 0.9148, 0.9526, 0.8746, 0.6582, 0.9229, 0.7542, Acc_f: 0.1332, Acc_r: 0.8806\n",
      "[train] epoch 13, batch 29, loss 1.9461179971694946\n",
      "0.9644, 0.9910, 0.9079, 0.1430, 0.9187, 0.9560, 0.8796, 0.6731, 0.9241, 0.7824, Acc_f: 0.1430, Acc_r: 0.8886\n",
      "[train] epoch 14, batch 29, loss 1.9052850008010864\n",
      "0.9633, 0.9908, 0.9007, 0.1228, 0.9180, 0.9513, 0.8730, 0.6563, 0.9241, 0.7724, Acc_f: 0.1228, Acc_r: 0.8833\n",
      "====================================================================================================================================================================================\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "[train] epoch 0, batch 29, loss 2.0428261756896973\n",
      "0.6319, 0.9614, 0.6086, 0.2641, 0.6801, 0.9719, 0.7157, 0.3789, 0.6042, 0.1724, Acc_f: 0.2641, Acc_r: 0.6361\n",
      "[train] epoch 1, batch 29, loss 1.9898204803466797\n",
      "0.5333, 0.9874, 0.5074, 0.3845, 0.6385, 0.9375, 0.6667, 0.2422, 0.6343, 0.0934, Acc_f: 0.3845, Acc_r: 0.5823\n",
      "[train] epoch 2, batch 29, loss 2.0132312774658203\n",
      "0.4971, 0.9878, 0.4900, 0.2679, 0.6005, 0.9400, 0.6692, 0.2021, 0.6627, 0.0771, Acc_f: 0.2679, Acc_r: 0.5696\n",
      "[train] epoch 3, batch 29, loss 1.9655383825302124\n",
      "0.5321, 0.9886, 0.5888, 0.3622, 0.6258, 0.9362, 0.6818, 0.2452, 0.6952, 0.1047, Acc_f: 0.3622, Acc_r: 0.5998\n",
      "[train] epoch 4, batch 29, loss 2.0449795722961426\n",
      "0.5522, 0.9888, 0.6177, 0.2915, 0.6500, 0.9337, 0.6945, 0.2585, 0.7127, 0.1191, Acc_f: 0.2915, Acc_r: 0.6141\n",
      "[train] epoch 5, batch 29, loss 1.9534279108047485\n",
      "0.5619, 0.9898, 0.6761, 0.3858, 0.6734, 0.9203, 0.6985, 0.2803, 0.7319, 0.1411, Acc_f: 0.3858, Acc_r: 0.6304\n",
      "[train] epoch 6, batch 29, loss 1.9701082706451416\n",
      "0.5516, 0.9910, 0.6674, 0.3650, 0.6869, 0.9157, 0.6960, 0.2823, 0.7512, 0.1467, Acc_f: 0.3650, Acc_r: 0.6321\n",
      "[train] epoch 7, batch 29, loss 1.9277366399765015\n",
      "0.5803, 0.9902, 0.7055, 0.3269, 0.7253, 0.9211, 0.7248, 0.3462, 0.7735, 0.1718, Acc_f: 0.3269, Acc_r: 0.6599\n",
      "[train] epoch 8, batch 29, loss 1.8043614625930786\n",
      "0.6026, 0.9900, 0.7182, 0.3668, 0.7269, 0.9224, 0.7258, 0.3754, 0.7831, 0.1893, Acc_f: 0.3668, Acc_r: 0.6704\n",
      "[train] epoch 9, batch 29, loss 1.9749852418899536\n",
      "0.6038, 0.9910, 0.7250, 0.3123, 0.7507, 0.9216, 0.7279, 0.3774, 0.8054, 0.2006, Acc_f: 0.3123, Acc_r: 0.6781\n",
      "[train] epoch 10, batch 29, loss 1.9836114645004272\n",
      "0.6170, 0.9886, 0.7503, 0.2856, 0.7459, 0.9258, 0.7410, 0.4131, 0.8187, 0.2088, Acc_f: 0.2856, Acc_r: 0.6899\n",
      "[train] epoch 11, batch 29, loss 1.839117169380188\n",
      "0.6416, 0.9867, 0.7927, 0.3633, 0.7677, 0.9253, 0.7481, 0.4433, 0.8277, 0.2257, Acc_f: 0.3633, Acc_r: 0.7065\n",
      "[train] epoch 12, batch 29, loss 1.9049720764160156\n",
      "0.6479, 0.9873, 0.8002, 0.3334, 0.7650, 0.9211, 0.7547, 0.4636, 0.8355, 0.2665, Acc_f: 0.3334, Acc_r: 0.7158\n",
      "[train] epoch 13, batch 29, loss 1.7944531440734863\n",
      "0.6686, 0.9851, 0.8250, 0.3629, 0.7967, 0.9165, 0.7658, 0.5290, 0.8458, 0.3166, Acc_f: 0.3629, Acc_r: 0.7388\n",
      "[train] epoch 14, batch 29, loss 1.8879263401031494\n",
      "0.6674, 0.9886, 0.8183, 0.3019, 0.7824, 0.9060, 0.7678, 0.5280, 0.8494, 0.3505, Acc_f: 0.3019, Acc_r: 0.7398\n"
     ]
    }
   ],
   "source": [
    "from agents.adv import FGSM\n",
    "\n",
    "def find_adjacent_cls(adv_agent, x, y):\n",
    "    x_adv = adv_agent.perturb(x, y)\n",
    "    adv_logits = model(x_adv)\n",
    "    adv_pred = torch.argmax(adv_logits.data, 1)\n",
    "    return adv_pred, x_adv\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "\n",
    "    adv_agent = FGSM(deepcopy(model), bound=0.5, norm=False, random_start=True, device='cuda')\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    print('==='*60)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            adv_pred, x_adv = find_adjacent_cls(adv_agent, x, y)\n",
    "            adv_y = torch.argmax(model(x_adv), dim=1).detach().cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, adv_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================================================================================================\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n",
      "[train] epoch 0, batch 29, loss 2.134913921356201\n",
      "0.6325, 0.9645, 0.5924, 0.2065, 0.7063, 0.9723, 0.7274, 0.3725, 0.6199, 0.1498, Acc_f: 0.2065, Acc_r: 0.6375\n",
      "[train] epoch 1, batch 29, loss 2.041485071182251\n",
      "0.5476, 0.9873, 0.5577, 0.2738, 0.6595, 0.9354, 0.6935, 0.2744, 0.6614, 0.1135, Acc_f: 0.2738, Acc_r: 0.6034\n",
      "[train] epoch 2, batch 29, loss 1.991804599761963\n",
      "0.5126, 0.9890, 0.5421, 0.3123, 0.6377, 0.9299, 0.6748, 0.2382, 0.6849, 0.0903, Acc_f: 0.3123, Acc_r: 0.5888\n",
      "[train] epoch 3, batch 29, loss 1.968560814857483\n",
      "0.5178, 0.9910, 0.5401, 0.2890, 0.6457, 0.9195, 0.6884, 0.2402, 0.7120, 0.1034, Acc_f: 0.2890, Acc_r: 0.5953\n",
      "[train] epoch 4, batch 29, loss 1.9913945198059082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5487, 0.9914, 0.6035, 0.2856, 0.6829, 0.9123, 0.6970, 0.2759, 0.7337, 0.1417, Acc_f: 0.2856, Acc_r: 0.6208\n",
      "[train] epoch 5, batch 29, loss 1.9811981916427612\n",
      "0.5677, 0.9918, 0.6226, 0.2720, 0.6932, 0.9102, 0.7248, 0.2942, 0.7651, 0.1661, Acc_f: 0.2720, Acc_r: 0.6373\n",
      "[train] epoch 6, batch 29, loss 1.9128860235214233\n",
      "0.5786, 0.9912, 0.6775, 0.2689, 0.7158, 0.9195, 0.7350, 0.3289, 0.7801, 0.1812, Acc_f: 0.2689, Acc_r: 0.6564\n",
      "[train] epoch 7, batch 29, loss 1.8777530193328857\n",
      "0.5958, 0.9924, 0.6763, 0.2644, 0.7582, 0.9119, 0.7516, 0.3556, 0.8006, 0.1925, Acc_f: 0.2644, Acc_r: 0.6705\n",
      "[train] epoch 8, batch 29, loss 1.842572569847107\n",
      "0.6365, 0.9908, 0.7344, 0.3591, 0.7772, 0.9203, 0.7683, 0.4131, 0.8169, 0.2351, Acc_f: 0.3591, Acc_r: 0.6992\n",
      "[train] epoch 9, batch 29, loss 1.9199988842010498\n",
      "0.6330, 0.9922, 0.7390, 0.2974, 0.7685, 0.9048, 0.7719, 0.4289, 0.8355, 0.2677, Acc_f: 0.2974, Acc_r: 0.7046\n",
      "[train] epoch 10, batch 29, loss 1.8599315881729126\n",
      "0.6514, 0.9886, 0.7816, 0.3439, 0.7883, 0.9207, 0.7764, 0.4720, 0.8500, 0.2671, Acc_f: 0.3439, Acc_r: 0.7218\n",
      "[train] epoch 11, batch 29, loss 1.7996186017990112\n",
      "0.6577, 0.9908, 0.7780, 0.3348, 0.7994, 0.9044, 0.7759, 0.4844, 0.8470, 0.2940, Acc_f: 0.3348, Acc_r: 0.7257\n",
      "[train] epoch 12, batch 29, loss 1.9656484127044678\n",
      "0.6760, 0.9896, 0.8062, 0.3168, 0.8070, 0.9052, 0.7931, 0.5171, 0.8536, 0.3530, Acc_f: 0.3168, Acc_r: 0.7445\n",
      "[train] epoch 13, batch 29, loss 1.8703277111053467\n",
      "0.6869, 0.9896, 0.8161, 0.2745, 0.8070, 0.8972, 0.7916, 0.5379, 0.8536, 0.3755, Acc_f: 0.2745, Acc_r: 0.7506\n",
      "[train] epoch 14, batch 29, loss 1.9712308645248413\n",
      "0.7231, 0.9873, 0.8494, 0.2786, 0.8181, 0.9077, 0.8113, 0.5899, 0.8620, 0.4326, Acc_f: 0.2786, Acc_r: 0.7757\n"
     ]
    }
   ],
   "source": [
    "from agents.adv import FGSM\n",
    "\n",
    "def find_adjacent_cls(adv_agent, x, y):\n",
    "    x_adv = adv_agent.perturb(x, y)\n",
    "    adv_logits = model(x_adv)\n",
    "    adv_pred = torch.argmax(adv_logits.data, 1)\n",
    "    return adv_pred, x_adv\n",
    "\n",
    "for i in range(2, 3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "\n",
    "    adv_agent = FGSM(deepcopy(model), bound=0.5, norm=False, random_start=True, device='cuda')\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    print('==='*60)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.001)\n",
    "\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "            \n",
    "    model.eval()\n",
    "    for ep in range(15):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            adv_pred, x_adv = find_adjacent_cls(adv_agent, x, y)\n",
    "            adv_y = torch.argmax(model(x_adv), dim=1).detach().cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, adv_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary Unlearning Acc_f: 4.03 \\pm 1.87\n",
      "Boundary Unlearning Acc_r: 80.99 \\pm 4.01\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.8015, 0.8627,  0.7656])\n",
    "Acc_f = 100*np.array([0.0219, 0.0659,  0.0330])\n",
    "print(f'Boundary Unlearning Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Boundary Unlearning Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "[train] epoch 0, batch 29, loss -0.09847826510667801\n",
      "0.9662, 0.9678, 0.9670, 0.9334, 0.9683, 0.9513, 0.9474, 0.9470, 0.9434, 0.9404, Acc_f: 0.9334, Acc_r: 0.9554\n",
      "[train] epoch 1, batch 29, loss -0.08678672462701797\n",
      "0.9662, 0.9680, 0.9677, 0.9303, 0.9687, 0.9526, 0.9479, 0.9475, 0.9452, 0.9411, Acc_f: 0.9303, Acc_r: 0.9561\n",
      "[train] epoch 2, batch 29, loss -0.03984031826257706\n",
      "0.9667, 0.9684, 0.9682, 0.9251, 0.9687, 0.9547, 0.9484, 0.9485, 0.9458, 0.9404, Acc_f: 0.9251, Acc_r: 0.9567\n",
      "[train] epoch 3, batch 29, loss -0.11497019976377487\n",
      "0.9667, 0.9686, 0.9684, 0.9198, 0.9687, 0.9568, 0.9494, 0.9490, 0.9446, 0.9411, Acc_f: 0.9198, Acc_r: 0.9570\n",
      "[train] epoch 4, batch 29, loss -0.08632414042949677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9667, 0.9686, 0.9691, 0.9129, 0.9691, 0.9581, 0.9494, 0.9490, 0.9446, 0.9411, Acc_f: 0.9129, Acc_r: 0.9573\n",
      "[train] epoch 5, batch 29, loss -0.15785294771194458\n",
      "0.9667, 0.9690, 0.9696, 0.9011, 0.9703, 0.9597, 0.9494, 0.9495, 0.9452, 0.9411, Acc_f: 0.9011, Acc_r: 0.9578\n",
      "[train] epoch 6, batch 29, loss -0.16154815256595612\n",
      "0.9667, 0.9694, 0.9694, 0.8938, 0.9699, 0.9618, 0.9509, 0.9495, 0.9464, 0.9417, Acc_f: 0.8938, Acc_r: 0.9584\n",
      "[train] epoch 7, batch 29, loss -0.16644635796546936\n",
      "0.9667, 0.9692, 0.9694, 0.8848, 0.9703, 0.9639, 0.9504, 0.9490, 0.9470, 0.9423, Acc_f: 0.8848, Acc_r: 0.9587\n",
      "[train] epoch 8, batch 29, loss -0.12518297135829926\n",
      "0.9667, 0.9692, 0.9704, 0.8730, 0.9707, 0.9648, 0.9504, 0.9500, 0.9464, 0.9423, Acc_f: 0.8730, Acc_r: 0.9590\n",
      "[train] epoch 9, batch 29, loss -0.15574957430362701\n",
      "0.9667, 0.9696, 0.9708, 0.8543, 0.9707, 0.9648, 0.9504, 0.9500, 0.9482, 0.9429, Acc_f: 0.8543, Acc_r: 0.9594\n",
      "[train] epoch 10, batch 29, loss -0.38853782415390015\n",
      "0.9667, 0.9700, 0.9701, 0.8338, 0.9703, 0.9669, 0.9504, 0.9495, 0.9482, 0.9429, Acc_f: 0.8338, Acc_r: 0.9594\n",
      "[train] epoch 11, batch 29, loss -0.39750936627388\n",
      "0.9662, 0.9704, 0.9701, 0.8022, 0.9715, 0.9698, 0.9494, 0.9500, 0.9470, 0.9429, Acc_f: 0.8022, Acc_r: 0.9597\n",
      "[train] epoch 12, batch 29, loss -0.7545914649963379\n",
      "0.9662, 0.9700, 0.9694, 0.7509, 0.9687, 0.9732, 0.9489, 0.9525, 0.9440, 0.9411, Acc_f: 0.7509, Acc_r: 0.9593\n",
      "[train] epoch 13, batch 29, loss -0.9039138555526733\n",
      "0.9662, 0.9698, 0.9684, 0.6790, 0.9667, 0.9740, 0.9428, 0.9510, 0.9392, 0.9404, Acc_f: 0.6790, Acc_r: 0.9576\n",
      "[train] epoch 14, batch 29, loss -1.2228697538375854\n",
      "0.9639, 0.9661, 0.9655, 0.5451, 0.9592, 0.9782, 0.9332, 0.9475, 0.9235, 0.9379, Acc_f: 0.5451, Acc_r: 0.9528\n",
      "[train] epoch 15, batch 29, loss -3.4161365032196045\n",
      "0.9536, 0.9545, 0.9610, 0.2561, 0.9247, 0.9862, 0.8746, 0.9396, 0.8898, 0.9304, Acc_f: 0.2561, Acc_r: 0.9349\n",
      "[train] epoch 16, batch 29, loss -9.548357963562012\n",
      "0.8538, 0.7862, 0.8764, 0.0007, 0.5957, 0.9966, 0.4416, 0.7697, 0.6175, 0.7392, Acc_f: 0.0007, Acc_r: 0.7419\n",
      "[train] epoch 17, batch 29, loss -27.406435012817383\n",
      "0.0442, 0.0004, 0.1412, 0.0000, 0.0008, 1.0000, 0.0000, 0.0000, 0.0012, 0.0019, Acc_f: 0.0000, Acc_r: 0.1322\n",
      "[train] epoch 18, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "\n",
      "\n",
      "\n",
      "[train] epoch 0, batch 29, loss -0.15120230615139008\n",
      "0.9644, 0.9659, 0.9725, 0.9146, 0.9647, 0.9652, 0.9459, 0.9475, 0.9452, 0.9486, Acc_f: 0.9146, Acc_r: 0.9578\n",
      "[train] epoch 1, batch 29, loss -0.17827558517456055\n",
      "0.9644, 0.9663, 0.9725, 0.9094, 0.9647, 0.9664, 0.9469, 0.9475, 0.9446, 0.9486, Acc_f: 0.9094, Acc_r: 0.9580\n",
      "[train] epoch 2, batch 29, loss -0.12965668737888336\n",
      "0.9644, 0.9669, 0.9732, 0.9035, 0.9647, 0.9669, 0.9479, 0.9480, 0.9446, 0.9492, Acc_f: 0.9035, Acc_r: 0.9584\n",
      "[train] epoch 3, batch 29, loss -0.157332643866539\n",
      "0.9650, 0.9671, 0.9732, 0.8973, 0.9651, 0.9685, 0.9484, 0.9480, 0.9428, 0.9498, Acc_f: 0.8973, Acc_r: 0.9587\n",
      "[train] epoch 4, batch 29, loss -0.22276049852371216\n",
      "0.9644, 0.9671, 0.9730, 0.8879, 0.9647, 0.9690, 0.9479, 0.9485, 0.9428, 0.9505, Acc_f: 0.8879, Acc_r: 0.9586\n",
      "[train] epoch 5, batch 29, loss -0.19338998198509216\n",
      "0.9644, 0.9669, 0.9735, 0.8768, 0.9647, 0.9702, 0.9494, 0.9480, 0.9422, 0.9505, Acc_f: 0.8768, Acc_r: 0.9589\n",
      "[train] epoch 6, batch 29, loss -0.2139657884836197\n",
      "0.9644, 0.9671, 0.9740, 0.8643, 0.9647, 0.9715, 0.9489, 0.9490, 0.9428, 0.9511, Acc_f: 0.8643, Acc_r: 0.9593\n",
      "[train] epoch 7, batch 29, loss -0.30787765979766846\n",
      "0.9633, 0.9667, 0.9740, 0.8484, 0.9659, 0.9719, 0.9489, 0.9485, 0.9428, 0.9524, Acc_f: 0.8484, Acc_r: 0.9594\n",
      "[train] epoch 8, batch 29, loss -0.29296308755874634\n",
      "0.9633, 0.9655, 0.9740, 0.8206, 0.9659, 0.9732, 0.9489, 0.9470, 0.9410, 0.9524, Acc_f: 0.8206, Acc_r: 0.9590\n",
      "[train] epoch 9, batch 29, loss -0.3711162209510803\n",
      "0.9633, 0.9653, 0.9732, 0.7772, 0.9659, 0.9778, 0.9474, 0.9455, 0.9367, 0.9530, Acc_f: 0.7772, Acc_r: 0.9587\n",
      "[train] epoch 10, batch 29, loss -0.735276460647583\n",
      "0.9622, 0.9647, 0.9718, 0.6978, 0.9627, 0.9803, 0.9433, 0.9425, 0.9283, 0.9517, Acc_f: 0.6978, Acc_r: 0.9564\n",
      "[train] epoch 11, batch 29, loss -1.3155769109725952\n",
      "0.9541, 0.9635, 0.9667, 0.5465, 0.9564, 0.9841, 0.9337, 0.9366, 0.9175, 0.9448, Acc_f: 0.5465, Acc_r: 0.9508\n",
      "[train] epoch 12, batch 29, loss -3.1125564575195312\n",
      "0.9369, 0.9484, 0.9412, 0.2401, 0.9243, 0.9891, 0.8842, 0.9059, 0.8614, 0.9166, Acc_f: 0.2401, Acc_r: 0.9231\n",
      "[train] epoch 13, batch 29, loss -10.754790306091309\n",
      "0.8045, 0.7411, 0.7248, 0.0000, 0.6195, 0.9966, 0.4188, 0.5577, 0.5620, 0.6345, Acc_f: 0.0000, Acc_r: 0.6733\n",
      "[train] epoch 14, batch 29, loss -60.84077835083008\n",
      "0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 15, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 16, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 17, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 18, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "\n",
      "\n",
      "\n",
      "[train] epoch 0, batch 29, loss -0.0362238772213459\n",
      "0.9507, 0.9725, 0.9689, 0.9341, 0.9564, 0.9413, 0.9454, 0.9292, 0.9446, 0.9392, Acc_f: 0.9341, Acc_r: 0.9498\n",
      "[train] epoch 1, batch 29, loss -0.027445128187537193\n",
      "0.9507, 0.9731, 0.9689, 0.9275, 0.9560, 0.9425, 0.9464, 0.9297, 0.9446, 0.9392, Acc_f: 0.9275, Acc_r: 0.9501\n",
      "[train] epoch 2, batch 29, loss -0.07867727428674698\n",
      "0.9507, 0.9739, 0.9696, 0.9216, 0.9564, 0.9451, 0.9484, 0.9302, 0.9434, 0.9392, Acc_f: 0.9216, Acc_r: 0.9508\n",
      "[train] epoch 3, batch 29, loss -0.13879597187042236\n",
      "0.9513, 0.9741, 0.9699, 0.9122, 0.9564, 0.9501, 0.9484, 0.9302, 0.9428, 0.9386, Acc_f: 0.9122, Acc_r: 0.9513\n",
      "[train] epoch 4, batch 29, loss -0.06847184151411057\n",
      "0.9507, 0.9737, 0.9704, 0.9011, 0.9560, 0.9539, 0.9474, 0.9307, 0.9428, 0.9392, Acc_f: 0.9011, Acc_r: 0.9516\n",
      "[train] epoch 5, batch 29, loss -0.20932306349277496\n",
      "0.9495, 0.9741, 0.9713, 0.8845, 0.9556, 0.9576, 0.9494, 0.9302, 0.9410, 0.9379, Acc_f: 0.8845, Acc_r: 0.9519\n",
      "[train] epoch 6, batch 29, loss -0.15930253267288208\n",
      "0.9495, 0.9743, 0.9708, 0.8643, 0.9552, 0.9631, 0.9504, 0.9307, 0.9404, 0.9367, Acc_f: 0.8643, Acc_r: 0.9523\n",
      "[train] epoch 7, batch 29, loss -0.3524961769580841\n",
      "0.9478, 0.9759, 0.9711, 0.8369, 0.9544, 0.9690, 0.9499, 0.9277, 0.9373, 0.9367, Acc_f: 0.8369, Acc_r: 0.9522\n",
      "[train] epoch 8, batch 29, loss -0.560844361782074\n",
      "0.9427, 0.9751, 0.9696, 0.7679, 0.9481, 0.9711, 0.9479, 0.9282, 0.9295, 0.9354, Acc_f: 0.7679, Acc_r: 0.9497\n",
      "[train] epoch 9, batch 29, loss -1.2877283096313477\n",
      "0.9358, 0.9712, 0.9643, 0.5951, 0.9370, 0.9794, 0.9393, 0.9203, 0.8958, 0.9292, Acc_f: 0.5951, Acc_r: 0.9414\n",
      "[train] epoch 10, batch 29, loss -6.227517604827881\n",
      "0.8578, 0.8963, 0.9031, 0.0861, 0.7713, 0.9920, 0.7653, 0.8019, 0.6976, 0.8577, Acc_f: 0.0861, Acc_r: 0.8381\n",
      "[train] epoch 11, batch 29, loss -36.92189025878906\n",
      "0.0006, 0.0000, 0.0465, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0031, Acc_f: 0.0000, Acc_r: 0.1167\n",
      "[train] epoch 12, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 13, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 14, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 15, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 16, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 17, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 18, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 19, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 20, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 21, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 22, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 23, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 24, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 25, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 26, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 27, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 28, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n",
      "[train] epoch 29, batch 29, loss nan\n",
      "1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, Acc_f: 0.0000, Acc_r: 0.1111\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('\\n\\n')\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.00008)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(30):\n",
    "        for batch, (x, y) in enumerate(unlearn_subset_loader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = -criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA Acc_f: 3.10 \\pm 3.90\n",
      "GA Acc_r: 75.11 \\pm 6.76\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.7419,  0.6733,  0.8381])\n",
    "Acc_f = 100*np.array([0.0070,  0.0000,  0.0861])\n",
    "\n",
    "print(f'GA Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'GA Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(device), orig_target.to(device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad2_acc += torch.mean(prob[:, y]) * p.grad.data.pow(2) \n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc /= len(train_loader)\n",
    "    \n",
    "def get_mean_var(args, p, alpha=1e-7):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3) \n",
    "    if p.size(0) == args.num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var \n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == args.num_classes:\n",
    "        mu[args.unlearn_class] = 0\n",
    "        var[args.unlearn_class] = 0.0001\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        var *= 10 \n",
    "    return mu, var\n",
    "\n",
    "def fisher_new(dataset, model):\n",
    "    for p in model.parameters():\n",
    "        p.data0 = copy.deepcopy(p.data.clone())\n",
    "    hessian(dataset, model)\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        mu, var = get_mean_var(args, p)\n",
    "        p.data = mu + var.sqrt() * torch.empty_like(p.data).normal_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9656, 0.9674, 0.9663, 0.9362, 0.9683, 0.9492, 0.9469, 0.9470, 0.9434, 0.9398, Acc_f: 0.9362, Acc_r: 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 58/114 [00:09<00:09,  6.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 114/114 [00:18<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9644, 0.9676, 0.9745, 0.0003, 0.9699, 0.9652, 0.9550, 0.9485, 0.9476, 0.9367, Acc_f: 0.0003, Acc_r: 0.9588\n",
      "0.9644, 0.9653, 0.9716, 0.9198, 0.9643, 0.9643, 0.9449, 0.9480, 0.9458, 0.9480, Acc_f: 0.9198, Acc_r: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 114/114 [00:18<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9667, 0.9718, 0.9759, 0.0413, 0.9675, 0.9727, 0.9489, 0.9411, 0.9482, 0.9524, Acc_f: 0.0413, Acc_r: 0.9606\n",
      "0.9513, 0.9723, 0.9687, 0.9410, 0.9568, 0.9379, 0.9449, 0.9297, 0.9434, 0.9386, Acc_f: 0.9410, Acc_r: 0.9493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 114/114 [00:18<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9536, 0.9767, 0.9773, 0.0805, 0.9584, 0.9572, 0.9459, 0.9217, 0.9380, 0.9373, Acc_f: 0.0805, Acc_r: 0.9518\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remain_dataset = torch.utils.data.Subset(train_loader.dataset, remain_indices)\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))  \n",
    "    test_by_class(model, test_loader, i=args.unlearn_class)\n",
    "    fisher_model = copy.deepcopy(model)\n",
    "    fisher_new(remain_dataset, fisher_model)\n",
    "    test_by_class(fisher_model, test_loader, i=args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher Acc_f: 2.53 \\pm 0.91\n",
      "Fisher Acc_r: 95.72 \\pm 0.30\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.9596,  0.9590, 0.9530])\n",
    "Acc_f = 100*np.array([0.0378,  0.0163, 0.0219])\n",
    "\n",
    "print(f'Fisher Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'Fisher Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hessian(dataset, model):\n",
    "    model.eval()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc = 0\n",
    "    \n",
    "    for data, orig_target in tqdm(train_loader):\n",
    "        data, orig_target = data.to(device), orig_target.to(device)\n",
    "        output = model(data)\n",
    "        prob = F.softmax(output, dim=-1).data\n",
    "\n",
    "        for y in range(output.shape[1]):\n",
    "            target = torch.empty_like(orig_target).fill_(y)\n",
    "            loss = loss_fn(output, target)\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            for p in model.parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.grad2_acc += torch.mean(prob[:, y]) * p.grad.data.pow(2) \n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad2_acc /= len(train_loader)\n",
    "    \n",
    "def get_mean_var(args, p, alpha=1.25e-7):\n",
    "    var = copy.deepcopy(1./(p.grad2_acc+1e-8))\n",
    "    var = var.clamp(max=1e3) \n",
    "    if p.size(0) == args.num_classes:\n",
    "        var = var.clamp(max=1e2)\n",
    "    var = alpha * var \n",
    "    \n",
    "    if p.ndim > 1:\n",
    "        var = var.mean(dim=1, keepdim=True).expand_as(p).clone()\n",
    "    mu = copy.deepcopy(p.data0.clone())\n",
    "\n",
    "    if p.size(0) == args.num_classes:\n",
    "        mu[unlearn_class] = 0\n",
    "        var[unlearn_class] = 0.0001\n",
    "        var *= 10\n",
    "    elif p.ndim == 1:\n",
    "        var *= 10 \n",
    "    return mu, var\n",
    "\n",
    "def fisher_new(dataset, model):\n",
    "    for p in model.parameters():\n",
    "        p.data0 = copy.deepcopy(p.data.clone())\n",
    "    hessian(dataset, model)\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        mu, var = get_mean_var(args, p)\n",
    "        p.data = mu + var.sqrt() * torch.empty_like(p.data).normal_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_class = 6\n",
    "remain_class = list(set(list(range(10))) -set([unlearn_class]))\n",
    "train_targets_list = np.array(train_loader.dataset.labels)\n",
    "remain_cls_indices = np.where(~np.isin(train_targets_list, unlearn_class))[0]\n",
    "cls_sampler = torch.utils.data.SubsetRandomSampler(remain_cls_indices)\n",
    "remain_loader = torch.utils.data.DataLoader(train_loader.dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            sampler=cls_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1688 [00:00<00:24, 68.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1688/1688 [00:20<00:00, 83.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8660, 0.1360, 0.7570, 0.7100, 0.9090, 0.9630, 0.0620, 0.3850, 0.9250, 0.9830, mean_acc: 0.7371\n"
     ]
    }
   ],
   "source": [
    "fisher_model = copy.deepcopy(model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "remain_dataset = torch.utils.data.Subset(train_loader.dataset, remain_cls_indices)\n",
    "fisher_new(remain_dataset, fisher_model)\n",
    "test_by_class(fisher_model, test_loader, i=unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8580, 0.9840, 0.9130, 0.9510, 0.8320, 0.9740, 0.7910, 0.9820, 0.9840, 0.9650, mean_acc: 0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1688/1688 [00:22<00:00, 73.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8380, 0.5230, 0.9720, 0.8160, 0.7210, 0.8370, 0.0020, 0.8360, 0.9460, 0.9870, mean_acc: 0.8307\n",
      "0.8740, 0.9850, 0.8920, 0.9380, 0.8150, 0.9700, 0.8120, 0.9810, 0.9850, 0.9680, mean_acc: 0.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1688/1688 [00:22<00:00, 75.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9110, 0.1970, 0.7550, 0.8350, 0.9700, 0.9510, 0.0290, 0.9380, 0.9000, 0.9560, mean_acc: 0.8237\n",
      "0.8180, 0.9840, 0.9150, 0.9330, 0.8350, 0.9630, 0.8460, 0.9640, 0.9870, 0.9780, mean_acc: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1688/1688 [00:20<00:00, 82.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900, 0.8760, 0.6560, 0.8230, 0.8000, 0.4780, 0.0000, 0.1040, 0.7990, 0.8450, mean_acc: 0.7079\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_dataloader(args)\n",
    "remain_dataset = torch.utils.data.Subset(train_loader.dataset, remain_cls_indices)\n",
    "\n",
    "for i in range(1,4):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}_{i}.pth'))\n",
    "    test_by_class(model, test_loader, i=6)\n",
    "    fisher_model = copy.deepcopy(model)\n",
    "    fisher_new(remain_dataset, fisher_model)\n",
    "    test_by_class(fisher_model, test_loader, i=unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain 78.7433-5.6311\n",
      "Forget 1.0333-1.3225\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.8307, 0.8237, 0.7079])\n",
    "Acc_f = 100*np.array([0.0020, 0.0290, 0.0000])\n",
    "\n",
    "print(f'Remain {np.mean(Acc_r):.4f}-{np.std(Acc_r):.4f}')\n",
    "print(f'Forget {np.mean(Acc_f):.4f}-{np.std(Acc_f):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SalUn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/train_32x32.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "arxiv_name = 'original_model_12-19-21-49'\n",
    "train_loader, val_loader, test_loader = get_dataloader(args)\n",
    "remain_train_loader, unlearn_train_loader = split_2_remain_unlearn(args, train_loader)\n",
    "remain_val_loader, unlearn_val_loader = split_2_remain_unlearn(args, val_loader)\n",
    "remain_test_loader, unlearn_test_loader = split_2_remain_unlearn(args, test_loader)\n",
    "\n",
    "remain_class = np.setdiff1d(np.arange(args.num_classes), args.unlearn_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# create saliency map\n",
    "def save_gradient_ratio(unlearn_train_loader, model, criterion, args, seed):\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        args.unlearn_lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    gradients = {}\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        gradients[name] = 0\n",
    "\n",
    "    for i, (image, target) in enumerate(unlearn_train_loader):\n",
    "        image = image.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output_clean = model(image)\n",
    "        loss = - criterion(output_clean, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    gradients[name] += param.grad.data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name in gradients:\n",
    "            gradients[name] = torch.abs_(gradients[name])\n",
    "\n",
    "    threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    for i in threshold_list:\n",
    "        print(i)\n",
    "        sorted_dict_positions = {}\n",
    "        hard_dict = {}\n",
    "\n",
    "        # Concatenate all tensors into a single tensor\n",
    "        all_elements = - torch.cat([tensor.flatten() for tensor in gradients.values()])\n",
    "\n",
    "        # Calculate the threshold index for the top 10% elements\n",
    "        threshold_index = int(len(all_elements) * i)\n",
    "\n",
    "        # Calculate positions of all elements\n",
    "        positions = torch.argsort(all_elements)\n",
    "        ranks = torch.argsort(positions)\n",
    "\n",
    "        start_index = 0\n",
    "        for key, tensor in gradients.items():\n",
    "            num_elements = tensor.numel()\n",
    "            # tensor_positions = positions[start_index: start_index + num_elements]\n",
    "            tensor_ranks = ranks[start_index : start_index + num_elements]\n",
    "\n",
    "            sorted_positions = tensor_ranks.reshape(tensor.shape)\n",
    "            sorted_dict_positions[key] = sorted_positions\n",
    "\n",
    "            # Set the corresponding elements to 1\n",
    "            threshold_tensor = torch.zeros_like(tensor_ranks)\n",
    "            threshold_tensor[tensor_ranks < threshold_index] = 1\n",
    "            threshold_tensor = threshold_tensor.reshape(tensor.shape)\n",
    "            hard_dict[key] = threshold_tensor\n",
    "            start_index += num_elements\n",
    "\n",
    "        torch.save(hard_dict, f'./save/{args.dataset}/{args.model_name}/mask_threshold_{seed}_{i}.pt')\n",
    "\n",
    "\n",
    "args.unlearn_lr=0.01\n",
    "args.momentum=0.9\n",
    "args.weight_decay=5e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(3):\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    save_gradient_ratio(unlearn_train_loader, model, criterion, args, args.seeds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.9719, 0.9712, 0.9624, 0.9167, 0.9687, 0.9488, 0.9474, 0.9534, 0.9253, 0.9611, Acc_f: 0.9167, Acc_r: 0.9567\n",
      "[train] epoch 0, batch 29, loss 3.429487705230713\n",
      "0.9725, 0.9847, 0.9369, 0.5965, 0.9564, 0.9442, 0.9155, 0.8524, 0.8590, 0.9398, Acc_f: 0.5965, Acc_r: 0.9290\n",
      "[train] epoch 1, batch 29, loss 2.7819137573242188\n",
      "0.9644, 0.9876, 0.8224, 0.1235, 0.9465, 0.8612, 0.8209, 0.6652, 0.7042, 0.7574, Acc_f: 0.1235, Acc_r: 0.8366\n",
      "[train] epoch 2, batch 29, loss 2.459082841873169\n",
      "0.9587, 0.9880, 0.7510, 0.0416, 0.9405, 0.8112, 0.7699, 0.5760, 0.5970, 0.6502, Acc_f: 0.0416, Acc_r: 0.7825\n",
      "[train] epoch 3, batch 29, loss 2.4924559593200684\n",
      "0.9530, 0.9865, 0.7069, 0.0187, 0.9338, 0.7945, 0.7375, 0.5280, 0.5398, 0.5893, Acc_f: 0.0187, Acc_r: 0.7521\n",
      "[train] epoch 4, batch 29, loss 2.3723368644714355\n",
      "0.9518, 0.9867, 0.6855, 0.0090, 0.9255, 0.7961, 0.7117, 0.4963, 0.5078, 0.5310, Acc_f: 0.0090, Acc_r: 0.7325\n",
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.9639, 0.9722, 0.9658, 0.9129, 0.9691, 0.9480, 0.9646, 0.9287, 0.9253, 0.9473, Acc_f: 0.9129, Acc_r: 0.9539\n",
      "[train] epoch 0, batch 29, loss 4.971887111663818\n",
      "0.9627, 0.9808, 0.9629, 0.7255, 0.9699, 0.9551, 0.9605, 0.9128, 0.8958, 0.9555, Acc_f: 0.7255, Acc_r: 0.9507\n",
      "[train] epoch 1, batch 29, loss 3.044867992401123\n",
      "0.9627, 0.9831, 0.9402, 0.2425, 0.9687, 0.9358, 0.9130, 0.8747, 0.8036, 0.9329, Acc_f: 0.2425, Acc_r: 0.9239\n",
      "[train] epoch 2, batch 29, loss 2.7382283210754395\n",
      "0.9587, 0.9822, 0.9135, 0.0961, 0.9719, 0.9182, 0.8680, 0.8380, 0.7042, 0.8740, Acc_f: 0.0961, Acc_r: 0.8921\n",
      "[train] epoch 3, batch 29, loss 2.5985770225524902\n",
      "0.9536, 0.9788, 0.8980, 0.0507, 0.9734, 0.9111, 0.8397, 0.8078, 0.6301, 0.8257, Acc_f: 0.0507, Acc_r: 0.8687\n",
      "[train] epoch 4, batch 29, loss 2.5160415172576904\n",
      "0.9455, 0.9745, 0.8879, 0.0281, 0.9746, 0.9090, 0.8113, 0.7831, 0.5735, 0.7918, Acc_f: 0.0281, Acc_r: 0.8501\n",
      "==============================================================================================================================================================================================================================================================================================================================================================\n",
      "0.9616, 0.9645, 0.9655, 0.9094, 0.9734, 0.9534, 0.9560, 0.9376, 0.9361, 0.9461, Acc_f: 0.9094, Acc_r: 0.9549\n",
      "[train] epoch 0, batch 29, loss 3.1740972995758057\n",
      "0.9547, 0.9790, 0.8081, 0.3230, 0.9608, 0.8855, 0.8619, 0.8583, 0.7861, 0.8658, Acc_f: 0.3230, Acc_r: 0.8845\n",
      "[train] epoch 1, batch 29, loss 2.6202406883239746\n",
      "0.9364, 0.9820, 0.5715, 0.0850, 0.9501, 0.7479, 0.7167, 0.7603, 0.5807, 0.6991, Acc_f: 0.0850, Acc_r: 0.7716\n",
      "[train] epoch 2, batch 29, loss 2.46010422706604\n",
      "0.9358, 0.9810, 0.5177, 0.0371, 0.9501, 0.7097, 0.6500, 0.7107, 0.4717, 0.6589, Acc_f: 0.0371, Acc_r: 0.7317\n",
      "[train] epoch 3, batch 29, loss 2.4186747074127197\n",
      "0.9335, 0.9790, 0.5054, 0.0173, 0.9524, 0.7110, 0.6196, 0.6904, 0.3982, 0.6508, Acc_f: 0.0173, Acc_r: 0.7156\n",
      "[train] epoch 4, batch 29, loss 2.4043636322021484\n",
      "0.9323, 0.9769, 0.4852, 0.0087, 0.9532, 0.7131, 0.5867, 0.6672, 0.3512, 0.6339, Acc_f: 0.0087, Acc_r: 0.7000\n",
      "MIA acc: 0.98 \\pm 0.00\n"
     ]
    }
   ],
   "source": [
    "from agents.svc_mia import SVC_MIA\n",
    "indice = remain_train_loader.sampler.indices\n",
    "neg_size = len(test_loader.sampler) + len(val_loader.sampler.indices)\n",
    "balanced_indice = np.random.choice(indice, size=neg_size, replace=False)\n",
    "balanced_sampler = torch.utils.data.SubsetRandomSampler(balanced_indice)\n",
    "balanced_train_loader = torch.utils.data.DataLoader(remain_train_loader.dataset,\n",
    "                                                    batch_size=args.batch_size,\n",
    "                                                    sampler=balanced_sampler)\n",
    "\n",
    "threshold = 0.8\n",
    "MIA_acc = np.zeros(3)\n",
    "for i in range(3):\n",
    "    print(\"=======\"*50)\n",
    "    mask = torch.load(f'./save/{args.dataset}/{args.model_name}/mask_threshold_{args.seeds[i]}_{threshold}.pt')\n",
    "    model = get_model(args)\n",
    "    model.load_state_dict(torch.load(f'./save/{args.dataset}/{args.model_name}/{arxiv_name}_{args.seeds[i]}.pth'))\n",
    "    sgd_mr_model = deepcopy(model)\n",
    "    test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "    optimizer = torch.optim.SGD(sgd_mr_model.parameters(), lr=0.0001)\n",
    "    sgd_mr_model.train()\n",
    "    for m in sgd_mr_model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "\n",
    "    for ep in range(5):\n",
    "        for batch, (x, y) in enumerate(unlearn_train_loader):\n",
    "            x = x.cuda()\n",
    "            y = torch.from_numpy(np.random.choice(remain_class, size=x.shape[0])).cuda()\n",
    "            pred_y = sgd_mr_model(x)\n",
    "            loss = criterion(pred_y, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for name, param in sgd_mr_model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= mask[name]\n",
    "            optimizer.step()\n",
    "        print('[train] epoch {}, batch {}, loss {}'.format(ep, batch, loss))\n",
    "        test_by_class(sgd_mr_model, test_loader, i=args.unlearn_class)\n",
    "\n",
    "    MIA_acc[i] =  SVC_MIA(shadow_train=balanced_train_loader, \n",
    "            target_train=None, \n",
    "            target_test=unlearn_train_loader,\n",
    "            shadow_test=test_loader, \n",
    "            model=sgd_mr_model)\n",
    "print(f'MIA acc: {MIA_acc.mean():.2f} \\pm {MIA_acc.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalUn Acc_f: 3.56 \\pm 0.56\n",
      "SalUn Acc_r: 78.81 \\pm 4.85\n"
     ]
    }
   ],
   "source": [
    "Acc_r = 100*np.array([0.7825,  0.8501,  0.7317])\n",
    "Acc_f = 100*np.array([0.0416,  0.0281,  0.0371])\n",
    "\n",
    "print(f'SalUn Acc_f: {Acc_f.mean():.2f} \\pm {Acc_f.std():.2f}')\n",
    "print(f'SalUn Acc_r: {Acc_r.mean():.2f} \\pm {Acc_r.std():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
